#!/usr/bin/env python3
"""
æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼ˆ44æ¬¡å…ƒï¼‰ã‚’ONNXã«å¤‰æ›
AUC 0.838ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’EC2ã§ã®å®Ÿç”¨ã«æœ€é©åŒ–
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def convert_model_to_onnx():
    """æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›"""
    
    print("ğŸ”„ æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ONNXå¤‰æ›ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    model_dir = Path("models/improved_high_performance")
    metadata_file = model_dir / "metadata.json"
    
    if not metadata_file.exists():
        print("âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    with open(metadata_file, 'r') as f:
        metadata = json.load(f)
    
    print(f"ğŸ“‹ ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
    print(f"   ç‰¹å¾´é‡æ•°: {metadata['feature_count']}")
    print(f"   æœ€é«˜AUC: {metadata['performance']['random_forest']['auc_mean']:.3f}")
    print(f"   æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {metadata['best_model']}")
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    best_model_name = metadata['best_model']
    model_file = model_dir / f"{best_model_name}_model.pkl"
    
    if not model_file.exists():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")
        return False
    
    model = joblib.load(model_file)
    feature_names = metadata['feature_names']
    
    print(f"âœ… {best_model_name}ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
    print("ğŸ”§ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆä¸­...")
    
    try:
        import duckdb
        conn = duckdb.connect("data/historical_data.duckdb", read_only=True)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        query = """
        SELECT 
            timestamp,
            open,
            high,
            low,
            close,
            volume
        FROM klines_btcusdt 
        LIMIT 10000
        """
        
        sample_data = conn.execute(query).df()
        sample_data['timestamp'] = pd.to_datetime(sample_data['timestamp'])
        sample_data.set_index('timestamp', inplace=True)
        conn.close()
        
        # åŒã˜ç‰¹å¾´é‡ã‚’ç”Ÿæˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
        df = sample_data.copy()
        
        # åŸºæœ¬ç‰¹å¾´é‡
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['hl_ratio'] = (df['high'] - df['low']) / df['close']
        df['oc_ratio'] = (df['close'] - df['open']) / df['open']
        
        # çŸ­æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
        for period in [1, 3, 5, 10, 20]:
            df[f'return_{period}'] = df['close'].pct_change(period)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        for window in [5, 10, 20, 30]:
            df[f'vol_{window}'] = df['returns'].rolling(window).std()
            if window in [10, 20]:
                df[f'vol_ratio_{window}'] = df[f'vol_{window}'] / df[f'vol_{window}'].rolling(window*2).mean()
        
        # ç§»å‹•å¹³å‡
        for ma in [5, 10, 20, 30]:
            df[f'sma_{ma}'] = df['close'].rolling(ma).mean()
            df[f'price_vs_sma_{ma}'] = (df['close'] - df[f'sma_{ma}']) / df[f'sma_{ma}']
        
        # EMA
        for ema in [5, 12]:
            df[f'ema_{ema}'] = df['close'].ewm(span=ema).mean()
            df[f'price_vs_ema_{ema}'] = (df['close'] - df[f'ema_{ema}']) / df[f'ema_{ema}']
        
        # MACD
        df['ema_26'] = df['close'].ewm(span=26).mean()
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # RSI
        for rsi_period in [14, 21]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=rsi_period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_period).mean()
            rs = gain / (loss + 1e-8)
            df[f'rsi_{rsi_period}'] = 100 - (100 / (1 + rs))
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        bb_middle = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        bb_upper = bb_middle + (bb_std * 2)
        bb_lower = bb_middle - (bb_std * 2)
        df['bb_position_20'] = (df['close'] - bb_lower) / (bb_upper - bb_lower + 1e-8)
        df['bb_width_20'] = (bb_upper - bb_lower) / bb_middle
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ 
        for vol_ma in [10, 20]:
            df[f'volume_ma_{vol_ma}'] = df['volume'].rolling(vol_ma).mean()
            df[f'volume_ratio_{vol_ma}'] = df['volume'] / df[f'volume_ma_{vol_ma}']
        
        df['log_volume'] = np.log(df['volume'] + 1)
        df['volume_price_trend'] = df['volume_ratio_20'] * df['returns']
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
        for momentum_period in [3, 5, 10]:
            df[f'momentum_{momentum_period}'] = df['close'].pct_change(momentum_period)
        
        # ä¾¡æ ¼ä½ç½®
        for lookback in [20, 50]:
            df[f'price_percentile_{lookback}'] = df['close'].rolling(lookback).rank(pct=True)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        df['sma_50'] = df['close'].rolling(50).mean()
        df['trend_strength_short'] = (df['sma_5'] - df['sma_20']) / df['sma_20']
        df['trend_strength_long'] = (df['sma_20'] - df['sma_50']) / df['sma_50']
        
        # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ 
        df['high_vol_regime'] = (df['vol_20'] > df['vol_20'].rolling(100).quantile(0.8)).astype(int)
        df['low_vol_regime'] = (df['vol_20'] < df['vol_20'].rolling(100).quantile(0.2)).astype(int)
        df['trending_market'] = (abs(df['trend_strength_short']) > df['trend_strength_short'].rolling(50).quantile(0.7)).astype(int)
        
        # æ™‚é–“ç‰¹å¾´é‡
        df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)
        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)
        
        # ç‰¹å¾´é‡ã‚’é¸æŠ
        available_features = [f for f in feature_names if f in df.columns]
        print(f"âœ… åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡: {len(available_features)}/{len(feature_names)}")
        
        X_sample = df[available_features].copy()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        X_sample = X_sample.replace([np.inf, -np.inf], np.nan)
        X_sample = X_sample.ffill().fillna(0)
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ãƒ•ã‚£ãƒƒãƒˆ
        scaler = StandardScaler()
        scaler.fit(X_sample.dropna())
        
        print("âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä½œæˆå®Œäº†")
        
    except Exception as e:
        print(f"âš ï¸ å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä½œæˆã«å¤±æ•—: {e}")
        print("ğŸ’¡ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ï¼ˆå¹³å‡0ã€æ¨™æº–åå·®1ï¼‰
        scaler = StandardScaler()
        dummy_data = np.random.randn(1000, len(feature_names))
        scaler.fit(dummy_data)
    
    # ONNXå¤‰æ›
    try:
        print("ğŸ”„ ONNXå¤‰æ›ä¸­...")
        
        # skl2onnxã‚’ä½¿ç”¨ã—ã¦ONNXå¤‰æ›
        from skl2onnx import convert_sklearn
        from skl2onnx.common.data_types import FloatTensorType
        
        initial_type = [('float_input', FloatTensorType([None, len(feature_names)]))]
        onnx_model = convert_sklearn(model, initial_types=initial_type)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = Path("models/v3.1_improved")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ONNXãƒ¢ãƒ‡ãƒ«ä¿å­˜
        onnx_file = output_dir / "model.onnx"
        with open(onnx_file, "wb") as f:
            f.write(onnx_model.SerializeToString())
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        scaler_file = output_dir / "scaler.pkl"
        joblib.dump(scaler, scaler_file)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        updated_metadata = {
            **metadata,
            "model_format": "onnx",
            "onnx_conversion_date": pd.Timestamp.now().strftime("%Y%m%d_%H%M%S"),
            "input_shape": [None, len(feature_names)],
            "deployment_ready": True
        }
        
        metadata_file = output_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(updated_metadata, f, indent=2)
        
        print("âœ… ONNXå¤‰æ›å®Œäº†")
        print(f"ğŸ“ ä¿å­˜å ´æ‰€: {output_dir}")
        print(f"ğŸ¯ AUCæ€§èƒ½: {metadata['performance']['random_forest']['auc_mean']:.3f}")
        print(f"ğŸ”¢ å…¥åŠ›æ¬¡å…ƒ: {len(feature_names)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("=" * 80)
    print("ğŸš€ æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ« ONNXå¤‰æ›")
    print("=" * 80)
    
    success = convert_model_to_onnx()
    
    if success:
        print("\nâœ… å¤‰æ›å®Œäº† - EC2ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™å®Œäº†")
    else:
        print("\nâŒ å¤‰æ›å¤±æ•—")
    
    return success

if __name__ == "__main__":
    success = main()