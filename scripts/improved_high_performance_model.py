#!/usr/bin/env python3
"""
æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
- é©åˆ‡ãªãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆã‚ˆã‚Šé•·ã„æ™‚é–“è»¸ã¨ç¾å®Ÿçš„ãªé–¾å€¤ï¼‰
- ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- ä»¥å‰ã®æˆåŠŸäº‹ä¾‹ã®è¨­å®šã‚’æ´»ç”¨
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import duckdb
from typing import Dict, List, Tuple
import joblib
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score
from sklearn.utils.class_weight import compute_class_weight
import lightgbm as lgb
import catboost as cb
import warnings
warnings.filterwarnings('ignore')

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class ImprovedHighPerformanceModel:
    """æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, profit_threshold: float = 0.003):  # ã‚ˆã‚Šç¾å®Ÿçš„ãªé–¾å€¤
        self.profit_threshold = profit_threshold
        self.scaler = StandardScaler()
        self.models = {}
        self.feature_names = []
        
    def load_market_data(self, symbol: str = "BTCUSDT", limit: int = 50000) -> pd.DataFrame:
        """ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“Š {symbol}ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆæœ€å¤§{limit:,}ä»¶ï¼‰...")
        
        try:
            conn = duckdb.connect("data/historical_data.duckdb", read_only=True)
            
            query = f"""
            SELECT 
                timestamp,
                open,
                high,
                low,
                close,
                volume
            FROM klines_{symbol.lower()}
            WHERE timestamp >= '2024-01-01'
            ORDER BY timestamp ASC
            LIMIT {limit}
            """
            
            data = conn.execute(query).df()
            data['timestamp'] = pd.to_datetime(data['timestamp'])
            data.set_index('timestamp', inplace=True)
            
            conn.close()
            print(f"âœ… {len(data):,}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"æœŸé–“: {data.index.min()} ã‹ã‚‰ {data.index.max()}")
            
            return data
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()

    def create_comprehensive_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ç”Ÿæˆï¼ˆä»¥å‰ã®æˆåŠŸäº‹ä¾‹ã«åŸºã¥ãï¼‰"""
        print("ğŸ”§ åŒ…æ‹¬çš„ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
        
        df = data.copy()
        
        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['hl_ratio'] = (df['high'] - df['low']) / df['close']
        df['oc_ratio'] = (df['close'] - df['open']) / df['open']
        
        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå¤šãã®æœŸé–“ï¼‰
        for period in [1, 2, 3, 5, 10, 15, 20, 30, 60]:
            df[f'return_{period}'] = df['close'].pct_change(period)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡
        for window in [5, 10, 15, 20, 30, 60]:
            df[f'vol_{window}'] = df['returns'].rolling(window).std()
            df[f'vol_ratio_{window}'] = df[f'vol_{window}'] / df[f'vol_{window}'].rolling(window*2).mean()
        
        # ç§»å‹•å¹³å‡ã¨ãƒˆãƒ¬ãƒ³ãƒ‰
        for ma in [5, 10, 15, 20, 30, 50]:
            df[f'sma_{ma}'] = df['close'].rolling(ma).mean()
            df[f'price_vs_sma_{ma}'] = (df['close'] - df[f'sma_{ma}']) / df[f'sma_{ma}']
        
        # æŒ‡æ•°ç§»å‹•å¹³å‡
        for ema in [5, 12, 26]:
            df[f'ema_{ema}'] = df['close'].ewm(span=ema).mean()
            df[f'price_vs_ema_{ema}'] = (df['close'] - df[f'ema_{ema}']) / df[f'ema_{ema}']
        
        # MACD
        df['macd'] = df['ema_12'] - df['ema_26']
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # RSI (è¤‡æ•°æœŸé–“)
        for rsi_period in [14, 21]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=rsi_period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=rsi_period).mean()
            rs = gain / (loss + 1e-8)
            df[f'rsi_{rsi_period}'] = 100 - (100 / (1 + rs))
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        for bb_period in [20, 50]:
            bb_middle = df['close'].rolling(bb_period).mean()
            bb_std = df['close'].rolling(bb_period).std()
            bb_upper = bb_middle + (bb_std * 2)
            bb_lower = bb_middle - (bb_std * 2)
            df[f'bb_position_{bb_period}'] = (df['close'] - bb_lower) / (bb_upper - bb_lower + 1e-8)
            df[f'bb_width_{bb_period}'] = (bb_upper - bb_lower) / bb_middle
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡
        for vol_ma in [10, 20, 50]:
            df[f'volume_ma_{vol_ma}'] = df['volume'].rolling(vol_ma).mean()
            df[f'volume_ratio_{vol_ma}'] = df['volume'] / df[f'volume_ma_{vol_ma}']
        
        df['log_volume'] = np.log(df['volume'] + 1)
        df['volume_price_trend'] = df['volume_ratio_20'] * df['returns']
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™
        for momentum_period in [3, 5, 10, 20]:
            df[f'momentum_{momentum_period}'] = df['close'].pct_change(momentum_period)
        
        # ä¾¡æ ¼ä½ç½®æŒ‡æ¨™
        for lookback in [20, 50, 100]:
            df[f'price_percentile_{lookback}'] = df['close'].rolling(lookback).rank(pct=True)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        df['trend_strength_short'] = (df['sma_5'] - df['sma_20']) / df['sma_20']
        df['trend_strength_long'] = (df['sma_20'] - df['sma_50']) / df['sma_50']
        
        # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æŒ‡æ¨™
        df['high_vol_regime'] = (df['vol_20'] > df['vol_20'].rolling(100).quantile(0.8)).astype(int)
        df['low_vol_regime'] = (df['vol_20'] < df['vol_20'].rolling(100).quantile(0.2)).astype(int)
        df['trending_market'] = (abs(df['trend_strength_short']) > df['trend_strength_short'].rolling(50).quantile(0.7)).astype(int)
        
        # æ™‚é–“ç‰¹å¾´é‡
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        
        # æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ã‚’é¸æŠï¼ˆä»¥å‰ã®æˆåŠŸäº‹ä¾‹ã«åŸºã¥ãï¼‰
        important_features = [
            'returns', 'log_returns', 'hl_ratio', 'oc_ratio',
            'return_1', 'return_3', 'return_5', 'return_10', 'return_20',
            'vol_5', 'vol_10', 'vol_20', 'vol_30',
            'vol_ratio_10', 'vol_ratio_20',
            'price_vs_sma_5', 'price_vs_sma_10', 'price_vs_sma_20', 'price_vs_sma_30',
            'price_vs_ema_5', 'price_vs_ema_12',
            'macd', 'macd_hist',
            'rsi_14', 'rsi_21',
            'bb_position_20', 'bb_width_20',
            'volume_ratio_10', 'volume_ratio_20',
            'log_volume', 'volume_price_trend',
            'momentum_3', 'momentum_5', 'momentum_10',
            'price_percentile_20', 'price_percentile_50',
            'trend_strength_short', 'trend_strength_long',
            'high_vol_regime', 'low_vol_regime', 'trending_market',
            'hour_sin', 'hour_cos', 'is_weekend'
        ]
        
        # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨
        available_features = [f for f in important_features if f in df.columns]
        self.feature_names = available_features
        
        print(f"âœ… {len(available_features)}å€‹ã®é‡è¦ç‰¹å¾´é‡ã‚’é¸æŠ")
        
        return df

    def create_balanced_labels(self, data: pd.DataFrame, horizons: List[int] = [10, 15, 20]) -> pd.Series:
        """ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒ©ãƒ™ãƒ«ç”Ÿæˆ"""
        print(f"ğŸ¯ è¤‡æ•°æ™‚é–“è»¸ã§ã®ãƒ©ãƒ™ãƒ«ç”Ÿæˆä¸­...")
        
        transaction_cost = 0.0008  # ã‚ˆã‚Šç¾å®Ÿçš„
        
        labels = []
        
        for horizon in horizons:
            future_return = data['close'].pct_change(horizon).shift(-horizon)
            
            # æ–¹å‘æ€§é‡è¦–ã®ãƒ©ãƒ™ãƒ«
            long_profit = future_return - transaction_cost
            short_profit = -future_return - transaction_cost
            
            # ã‚ˆã‚Šç¾å®Ÿçš„ãªé–¾å€¤
            threshold = self.profit_threshold
            profitable = ((long_profit > threshold) | (short_profit > threshold))
            
            labels.append(profitable)
        
        # è¤‡æ•°æ™‚é–“è»¸ã§ã®åˆæ„
        combined_labels = pd.concat(labels, axis=1).any(axis=1).astype(int)
        
        print(f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†")
        print(f"   é™½æ€§ç‡: {combined_labels.mean():.2%}")
        print(f"   å„æ™‚é–“è»¸ã®é™½æ€§ç‡: {[f'{h}min: {l.mean():.2%}' for h, l in zip(horizons, labels)]}")
        
        return combined_labels

    def prepare_training_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        print("ğŸ—‚ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
        
        y = self.create_balanced_labels(data)
        X = data[self.feature_names].copy()
        
        # æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä½¿ç”¨
        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[valid_mask]
        y = y[valid_mask]
        
        # æ¬ æå€¤å‡¦ç†
        for col in X.columns:
            if X[col].dtype in ['float64', 'int64']:
                X[col] = X[col].fillna(X[col].median())
            else:
                X[col] = X[col].fillna(0)
        
        # ãƒ‡ãƒ¼ã‚¿ã®å“è³ªãƒã‚§ãƒƒã‚¯
        print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
        print(f"   ç‰¹å¾´é‡æ•°: {len(X.columns)}")
        print(f"   é™½æ€§ç‡: {y.mean():.2%}")
        print(f"   æœŸé–“: {X.index.min()} ã‹ã‚‰ {X.index.max()}")
        
        return X, y

    def train_balanced_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("ğŸ¤– ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))
        
        print(f"ã‚¯ãƒ©ã‚¹é‡ã¿: {class_weight_dict}")
        
        models = {
            'random_forest': RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'lightgbm': lgb.LGBMClassifier(
                n_estimators=300,
                learning_rate=0.05,
                max_depth=8,
                num_leaves=50,
                min_child_samples=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1,
                verbose=-1
            ),
            'catboost': cb.CatBoostClassifier(
                iterations=500,
                learning_rate=0.03,
                depth=8,
                class_weights=class_weight_dict,
                random_seed=42,
                verbose=False
            )
        }
        
        cv_results = {}
        tscv = TimeSeriesSplit(n_splits=3, test_size=int(len(X)*0.2))
        
        for name, model in models.items():
            print(f"ğŸ”„ {name}ã‚’è¨“ç·´ä¸­...")
            
            cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='roc_auc', n_jobs=1)
            
            model.fit(X, y)
            self.models[name] = model
            
            cv_results[name] = {
                'auc_mean': cv_scores.mean(),
                'auc_std': cv_scores.std()
            }
            
            print(f"   AUC: {cv_results[name]['auc_mean']:.3f}Â±{cv_results[name]['auc_std']:.3f}")
        
        # åŠ é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ
        print("ğŸ”„ åŠ é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆä¸­...")
        
        # æ€§èƒ½ã«åŸºã¥ãé‡ã¿
        weights = [cv_results[name]['auc_mean'] for name in models.keys()]
        ensemble_models = [(name, model) for name, model in self.models.items()]
        
        ensemble = VotingClassifier(
            estimators=ensemble_models, 
            voting='soft'
        )
        ensemble.fit(X, y)
        self.models['ensemble'] = ensemble
        
        ensemble_cv_scores = cross_val_score(ensemble, X, y, cv=tscv, scoring='roc_auc', n_jobs=1)
        cv_results['ensemble'] = {
            'auc_mean': ensemble_cv_scores.mean(),
            'auc_std': ensemble_cv_scores.std()
        }
        
        print(f"   ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«AUC: {cv_results['ensemble']['auc_mean']:.3f}Â±{cv_results['ensemble']['auc_std']:.3f}")
        
        return cv_results

    def comprehensive_evaluation(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """åŒ…æ‹¬çš„ãªè©•ä¾¡"""
        print("ğŸ“Š åŒ…æ‹¬çš„è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        best_model_name = max(self.models.keys(), 
                             key=lambda k: self.models[k].__class__.__name__ != 'NoneType' and
                             hasattr(self.models[k], 'predict_proba'))
        
        best_model = self.models.get('ensemble', self.models.get('catboost', self.models.get('random_forest')))
        
        y_proba = best_model.predict_proba(X)[:, 1]
        
        # é–¾å€¤åˆ¥è©•ä¾¡
        evaluation_results = {}
        
        for threshold in np.arange(0.3, 0.95, 0.05):
            threshold = round(threshold, 2)
            signals = (y_proba > threshold)
            
            if signals.sum() > 0:
                accuracy = y[signals].mean()
                signal_count = signals.sum()
                
                # å–å¼•é »åº¦è¨ˆç®—
                total_hours = (X.index.max() - X.index.min()).total_seconds() / 3600
                signals_per_day = signal_count / (total_hours / 24)
                
                evaluation_results[threshold] = {
                    'signal_count': int(signal_count),
                    'accuracy': float(accuracy),
                    'signals_per_day': float(signals_per_day),
                    'precision': float(accuracy),
                    'expected_daily_trades': max(0, int(signals_per_day))
                }
            else:
                evaluation_results[threshold] = {
                    'signal_count': 0,
                    'accuracy': 0,
                    'signals_per_day': 0,
                    'precision': 0,
                    'expected_daily_trades': 0
                }
        
        return evaluation_results

    def save_improved_model(self, cv_results: Dict, model_dir: str = "models/improved_high_performance"):
        """æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        model_path = Path(model_dir)
        model_path.mkdir(parents=True, exist_ok=True)
        
        # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        for name, model in self.models.items():
            model_file = model_path / f"{name}_model.pkl"
            joblib.dump(model, model_file)
            print(f"ğŸ’¾ {name}ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        import json
        from datetime import datetime
        
        best_model_name = max(cv_results.keys(), key=lambda k: cv_results[k]['auc_mean'])
        
        metadata = {
            "model_type": "improved_high_performance",
            "feature_count": len(self.feature_names),
            "feature_names": self.feature_names,
            "training_date": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "model_version": "3.1_improved",
            "best_model": best_model_name,
            "performance": cv_results,
            "config": {
                "profit_threshold": self.profit_threshold,
                "transaction_cost": 0.0008,
                "horizons": [10, 15, 20],
                "approach": "balanced_multi_horizon"
            }
        }
        
        metadata_file = model_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        return metadata


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("="*80)
    print("ğŸš€ æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
    print("="*80)
    
    model = ImprovedHighPerformanceModel(profit_threshold=0.003)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = model.load_market_data(limit=50000)
    if len(data) < 5000:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return
    
    # 2. ç‰¹å¾´é‡ç”Ÿæˆ
    data_with_features = model.create_comprehensive_features(data)
    
    # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X, y = model.prepare_training_data(data_with_features)
    if len(X) < 1000:
        print("âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return
    
    # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    cv_results = model.train_balanced_models(X, y)
    
    # 5. åŒ…æ‹¬çš„è©•ä¾¡
    evaluation_results = model.comprehensive_evaluation(X, y)
    
    # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    metadata = model.save_improved_model(cv_results)
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“Š æ”¹å–„ã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«çµæœ")
    print("="*80)
    
    print(f"\nğŸ”¢ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
    print(f"   ç‰¹å¾´é‡æ•°: {len(X.columns)}")
    print(f"   é™½æ€§ç‡: {y.mean():.2%}")
    print(f"   æœŸé–“: {X.index.min().strftime('%Y-%m-%d')} ã‹ã‚‰ {X.index.max().strftime('%Y-%m-%d')}")
    
    print(f"\nğŸ¯ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
    for model_name, scores in cv_results.items():
        status = "ğŸ†" if scores['auc_mean'] == max(s['auc_mean'] for s in cv_results.values()) else "  "
        print(f"   {status} {model_name.upper():15}: AUC = {scores['auc_mean']:.3f}Â±{scores['auc_std']:.3f}")
    
    best_auc = max(cv_results[k]['auc_mean'] for k in cv_results.keys())
    
    print(f"\nğŸ“ˆ é–¾å€¤åˆ¥å–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:")
    optimal_thresholds = []
    for threshold, result in evaluation_results.items():
        if result['signal_count'] > 0 and result['accuracy'] > 0.6:
            optimal_thresholds.append((threshold, result))
    
    # ä¸Šä½5ã¤ã®é–¾å€¤ã‚’è¡¨ç¤º
    optimal_thresholds.sort(key=lambda x: x[1]['accuracy'], reverse=True)
    for threshold, result in optimal_thresholds[:5]:
        print(f"   é–¾å€¤ {threshold}: {result['signal_count']}å›å–å¼•, "
              f"ç²¾åº¦ {result['accuracy']:.1%}, "
              f"1æ—¥ {result['expected_daily_trades']}å›")
    
    # æœ€çµ‚è©•ä¾¡
    if best_auc >= 0.75:
        print(f"\nâœ… å„ªç§€ãªæ€§èƒ½ (AUC {best_auc:.3f}) - ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨")
        deployment_status = "âœ… ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™å®Œäº†"
    elif best_auc >= 0.65:
        print(f"\nğŸŸ¨ è‰¯å¥½ãªæ€§èƒ½ (AUC {best_auc:.3f}) - æ¡ä»¶ä»˜ãã§ä½¿ç”¨å¯èƒ½")
        deployment_status = "ğŸŸ¨ æ…é‡ã«ãƒ‡ãƒ—ãƒ­ã‚¤"
    else:
        print(f"\nâŒ æ€§èƒ½ä¸è¶³ (AUC {best_auc:.3f}) - ã•ã‚‰ãªã‚‹æ”¹å–„å¿…è¦")
        deployment_status = "âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ä¸æ¨å¥¨"
    
    print(f"\nğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ¨å¥¨: {deployment_status}")
    print(f"ğŸ’¾ ä¿å­˜å ´æ‰€: models/improved_high_performance/")
    
    return model, cv_results, evaluation_results


if __name__ == "__main__":
    model, cv_results, evaluation_results = main()