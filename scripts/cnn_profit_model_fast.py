#!/usr/bin/env python3
"""
Fast CNN-based profit prediction model with optimized settings.
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import duckdb
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.common.logging import get_logger, setup_logging

setup_logging()
logger = get_logger(__name__)


def create_simple_features(data):
    """Create essential features only for faster processing."""
    # Core features
    data['returns'] = data['close'].pct_change()
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))
    
    # Essential momentum
    data['momentum_3'] = data['close'].pct_change(3)
    data['momentum_5'] = data['close'].pct_change(5) 
    data['momentum_10'] = data['close'].pct_change(10)
    
    # Key volatility
    data['volatility_10'] = data['returns'].rolling(10).std()
    data['volatility_20'] = data['returns'].rolling(20).std()
    
    # Critical moving averages
    data['sma_10'] = data['close'].rolling(10).mean()
    data['sma_20'] = data['close'].rolling(20).mean()
    data['price_to_sma_20'] = data['close'] / data['sma_20']
    
    # RSI
    delta = data['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / (loss + 1e-8)
    data['rsi'] = 100 - (100 / (1 + rs))
    
    # Volume
    data['volume_ratio'] = data['volume'] / data['volume'].rolling(20).mean()
    
    # High/Low
    data['hl_ratio'] = (data['high'] - data['low']) / data['close']
    
    return data


def create_sequences(data, features, sequence_length=30):
    """Create sequences with simpler target."""
    sequences = []
    targets = []
    
    for i in range(len(data) - sequence_length - 5):
        seq = data[features].iloc[i:i+sequence_length].values
        sequences.append(seq)
        
        # Simple target: profitable if next 5-bar return > 0.2%
        entry_price = data['close'].iloc[i + sequence_length]
        exit_price = data['close'].iloc[i + sequence_length + 5]
        pnl = (exit_price - entry_price) / entry_price
        targets.append(1 if pnl > 0.002 else 0)
    
    return np.array(sequences), np.array(targets)


def build_simple_cnn(input_shape):
    """Build simplified CNN model."""
    model = Sequential([
        Conv1D(filters=32, kernel_size=3, activation='relu', 
               input_shape=input_shape, padding='same'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        
        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),
        BatchNormalization(),
        GlobalAveragePooling1D(),
        Dropout(0.3),
        
        Dense(32, activation='relu'),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )
    
    return model


def train_fast_cnn():
    """Train CNN model quickly."""
    
    print("\n" + "="*80)
    print("ğŸš€ é«˜é€ŸCNNãƒ¢ãƒ‡ãƒ«è¨“ç·´")
    print("="*80)
    
    logger.info("Starting fast CNN training")
    
    # Load smaller dataset
    conn = duckdb.connect("data/historical_data.duckdb")
    query = """
    SELECT timestamp, open, high, low, close, volume
    FROM klines_btcusdt
    WHERE timestamp >= '2024-03-01' AND timestamp <= '2024-06-30'
    ORDER BY timestamp
    """
    
    data = conn.execute(query).df()
    data['timestamp'] = pd.to_datetime(data['timestamp'])
    data.set_index('timestamp', inplace=True)
    conn.close()
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(data)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    # Create features
    data = create_simple_features(data)
    data = data.dropna()
    
    print(f"ğŸ“Š ç‰¹å¾´é‡ä½œæˆå¾Œ: {len(data)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    # Select features
    feature_cols = ['returns', 'log_returns', 'momentum_3', 'momentum_5', 
                   'momentum_10', 'volatility_10', 'volatility_20', 
                   'price_to_sma_20', 'rsi', 'volume_ratio', 'hl_ratio']
    
    # Create sequences
    print("ğŸ“Š ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆä¸­...")
    X, y = create_sequences(data, feature_cols, sequence_length=30)
    print(f"âœ… {len(X)} ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆå®Œäº†")
    print(f"ğŸ“Š ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚¯ãƒ©ã‚¹æ¯”ç‡: {y.mean():.2%}")
    
    # Normalize
    scaler = StandardScaler()
    X_reshaped = X.reshape(-1, X.shape[-1])
    X_normalized = scaler.fit_transform(X_reshaped)
    X = X_normalized.reshape(X.shape)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²:")
    print(f"  è¨“ç·´: {len(X_train)}")
    print(f"  ãƒ†ã‚¹ãƒˆ: {len(X_test)}")
    
    # Build and train model
    print(f"\nğŸ§  CNNãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ä¸­...")
    model = build_simple_cnn((X.shape[1], X.shape[2]))
    
    # Early stopping
    early_stop = EarlyStopping(
        monitor='val_auc', 
        patience=10, 
        restore_best_weights=True,
        mode='max'
    )
    
    # Class weight
    pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])
    class_weight = {0: 1.0, 1: pos_weight}
    
    print(f"ğŸƒ è¨“ç·´é–‹å§‹...")
    history = model.fit(
        X_train, y_train,
        epochs=30,
        batch_size=64,
        validation_split=0.2,
        callbacks=[early_stop],
        class_weight=class_weight,
        verbose=1
    )
    
    # Evaluate
    test_loss, test_acc, test_auc = model.evaluate(X_test, y_test, verbose=0)
    print(f"\nâœ… ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"  ç²¾åº¦: {test_acc:.4f}")
    print(f"  AUC: {test_auc:.4f}")
    
    # Save model and scaler
    model.save("models/cnn_fast_model.h5")
    joblib.dump(scaler, "models/cnn_scaler.pkl")
    print(f"\nğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")
    
    # Quick backtest
    print(f"\n" + "="*80)
    print(f"ğŸ’° åç›Šæ€§ãƒ†ã‚¹ãƒˆ")
    print(f"="*80)
    
    # Load test data
    conn = duckdb.connect("data/historical_data.duckdb")
    test_query = """
    SELECT timestamp, open, high, low, close, volume
    FROM klines_btcusdt
    WHERE timestamp >= '2024-07-01' AND timestamp <= '2024-08-31'
    ORDER BY timestamp
    """
    
    test_data = conn.execute(test_query).df()
    test_data['timestamp'] = pd.to_datetime(test_data['timestamp'])
    test_data.set_index('timestamp', inplace=True)
    conn.close()
    
    # Create features
    test_data = create_simple_features(test_data)
    test_data = test_data.dropna()
    
    # Create sequences
    X_live, _ = create_sequences(test_data, feature_cols, sequence_length=30)
    X_live_reshaped = X_live.reshape(-1, X_live.shape[-1])
    X_live_normalized = scaler.transform(X_live_reshaped)
    X_live = X_live_normalized.reshape(X_live.shape)
    
    # Get predictions
    predictions = model.predict(X_live, verbose=0)
    
    # Simulate trading
    capital = 100000
    position_size = 0.015
    confidence_threshold = 0.7
    trades = []
    
    high_confidence_indices = np.where(predictions > confidence_threshold)[0]
    print(f"ğŸ“Š é«˜ä¿¡é ¼ã‚·ã‚°ãƒŠãƒ«: {len(high_confidence_indices)}")
    
    for idx in high_confidence_indices[:50]:  # Test first 50
        if idx + 30 + 5 >= len(test_data):
            continue
        
        entry_idx = idx + 30
        entry_price = test_data['close'].iloc[entry_idx]
        exit_price = test_data['close'].iloc[entry_idx + 5]
        
        momentum = test_data['momentum_5'].iloc[entry_idx]
        direction = 'long' if momentum > 0 else 'short'
        
        if direction == 'long':
            pnl_pct = (exit_price - entry_price) / entry_price
        else:
            pnl_pct = (entry_price - exit_price) / entry_price
        
        pnl_pct -= 0.001  # Lower fees
        
        position_value = capital * position_size
        pnl_dollar = position_value * pnl_pct
        
        trades.append({
            'pnl_dollar': pnl_dollar,
            'confidence': float(predictions[idx])
        })
        
        capital += pnl_dollar
    
    if trades:
        trades_df = pd.DataFrame(trades)
        total_return = (capital - 100000) / 100000 * 100
        win_rate = len(trades_df[trades_df['pnl_dollar'] > 0]) / len(trades_df) * 100
        avg_pnl = trades_df['pnl_dollar'].mean()
        
        print(f"\nğŸ“Š ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ (2ãƒ¶æœˆ):")
        print(f"  å–å¼•æ•°: {len(trades_df)}")
        print(f"  ç·åç›Šç‡: {total_return:.2f}%")
        print(f"  æœˆæ¬¡åç›Šç‡: {total_return/2:.2f}%")
        print(f"  å‹ç‡: {win_rate:.1f}%")
        print(f"  å¹³å‡å–å¼•æç›Š: ${avg_pnl:.2f}")
        
        if total_return > 0:
            print(f"\nğŸ‰ åç›Šæ€§é”æˆï¼")
            print(f"  æ¨å®šå¹´æ¬¡åç›Š: {total_return * 6:.1f}%")
            
            # Comparison with previous models
            print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
            print(f"  åˆæœŸæˆ¦ç•¥: -5.53% æœˆæ¬¡")
            print(f"  Random Forest: -0.01% æœˆæ¬¡")
            print(f"  CNN (TensorFlow): {total_return/2:.2f}% æœˆæ¬¡")
            
            improvement = (total_return/2) - (-5.53)
            print(f"\nğŸ“ˆ æ”¹å–„å¹…: {improvement:.2f}% (åˆæœŸæ¯”)")
        else:
            print(f"\nâš ï¸ ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")
    
    print(f"\nâœ… CNNå®Ÿè£…æˆåŠŸ:")
    print(f"  - TensorFlow 2.16.2 (Python 3.12)")
    print(f"  - Metal GPU ã‚µãƒãƒ¼ãƒˆæœ‰åŠ¹")
    print(f"  - AUC: {test_auc:.4f}")
    
    return model, test_auc


if __name__ == "__main__":
    # Verify environment
    print(f"TensorFlow version: {tf.__version__}")
    print(f"GPU devices: {tf.config.list_physical_devices('GPU')}")
    
    # Train model
    train_fast_cnn()