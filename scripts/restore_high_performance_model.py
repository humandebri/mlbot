#!/usr/bin/env python3
"""
ä»¥å‰ã®é«˜æ€§èƒ½ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆAUC 0.867ï¼‰ã‚’å¾©å…ƒã—ã€å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´ã™ã‚‹
35å€‹ã®å®Ÿè¨¼æ¸ˆã¿ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã€ãƒã‚¤ã‚ºã®å¤šã„ç‰¹å¾´é‡ã‚’é™¤å»
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import duckdb
from typing import Dict, List, Tuple
import joblib
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score
import lightgbm as lgb
import catboost as cb
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class HighPerformanceModelRestoration:
    """é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®å¾©å…ƒã¨å†è¨“ç·´"""
    
    def __init__(self, profit_threshold: float = 0.005):
        self.profit_threshold = profit_threshold
        self.scaler = StandardScaler()
        self.models = {}
        self.feature_names = []
        
    def load_market_data(self, symbol: str = "BTCUSDT", start_date: str = "2024-01-01") -> pd.DataFrame:
        """å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“Š {symbol}ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            conn = duckdb.connect("data/historical_data.duckdb", read_only=True)
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’ç¢ºèª
            tables = conn.execute("SHOW TABLES").fetchall()
            print(f"åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«: {[t[0] for t in tables]}")
            
            # BTCUSDTãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            query = f"""
            SELECT 
                timestamp,
                open,
                high,
                low,
                close,
                volume
            FROM klines_{symbol.lower()}
            WHERE timestamp >= '{start_date}'
            ORDER BY timestamp
            """
            
            data = conn.execute(query).df()
            data['timestamp'] = pd.to_datetime(data['timestamp'])
            data.set_index('timestamp', inplace=True)
            
            conn.close()
            print(f"âœ… {len(data):,}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
            
            return data
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            print("ğŸ“Š ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
            dates = pd.date_range(start_date, periods=10000, freq='5min')
            
            # ç¾å®Ÿçš„ãªä¾¡æ ¼å‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            np.random.seed(42)
            base_price = 50000
            returns = np.random.normal(0, 0.002, len(dates))
            returns = np.cumsum(returns)
            prices = base_price * np.exp(returns)
            
            data = pd.DataFrame({
                'timestamp': dates,
                'open': prices,
                'high': prices * (1 + np.abs(np.random.normal(0, 0.001, len(dates)))),
                'low': prices * (1 - np.abs(np.random.normal(0, 0.001, len(dates)))),
                'close': prices,
                'volume': np.random.lognormal(10, 1, len(dates))
            })
            
            data.set_index('timestamp', inplace=True)
            return data

    def create_proven_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """å®Ÿè¨¼æ¸ˆã¿ã®35å€‹ã®é«˜å“è³ªç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        print("ğŸ”§ å®Ÿè¨¼æ¸ˆã¿ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
        
        df = data.copy()
        
        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡ (4å€‹)
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['hl_ratio'] = (df['high'] - df['low']) / df['close']
        df['oc_ratio'] = (df['close'] - df['open']) / df['open']
        
        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¿ãƒ¼ãƒ³ (6å€‹)
        for period in [1, 3, 5, 10, 15, 30]:
            df[f'return_{period}'] = df['close'].pct_change(period)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡ (4å€‹)
        for window in [5, 10, 20, 30]:
            df[f'vol_{window}'] = df['returns'].rolling(window).std()
        
        # ç§»å‹•å¹³å‡ã¨ä¾¡æ ¼æ¯”è¼ƒ (6å€‹)
        for ma in [5, 10, 20]:
            df[f'sma_{ma}'] = df['close'].rolling(ma).mean()
            df[f'price_vs_sma_{ma}'] = (df['close'] - df[f'sma_{ma}']) / df[f'sma_{ma}']
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™ (3å€‹)
        df['momentum_3'] = df['close'].pct_change(3)
        df['momentum_5'] = df['close'].pct_change(5)
        df['momentum_10'] = df['close'].pct_change(10)
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡ (3å€‹)
        df['volume_ma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_ma']
        df['log_volume'] = np.log(df['volume'] + 1)
        
        # RSI (1å€‹)
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / (loss + 1e-8)
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½® (1å€‹)
        bb_middle = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        bb_upper = bb_middle + (bb_std * 2)
        bb_lower = bb_middle - (bb_std * 2)
        df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower + 1e-8)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ (2å€‹)
        df['trend_strength'] = (df['sma_5'] - df['sma_20']) / df['sma_20']
        df['price_above_ma'] = (df['close'] > df['sma_20']).astype(int)
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ -ä¾¡æ ¼ç›¸äº’ä½œç”¨ (1å€‹)
        df['volume_price_change'] = df['volume_ratio'] * abs(df['returns'])
        
        # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ  (2å€‹)
        df['high_vol'] = (df['vol_20'] > df['vol_20'].rolling(50).quantile(0.8)).astype(int)
        df['low_vol'] = (df['vol_20'] < df['vol_20'].rolling(50).quantile(0.2)).astype(int)
        
        # æ™‚é–“ç‰¹å¾´é‡ (2å€‹)
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        
        print("âœ… 35å€‹ã®å®Ÿè¨¼æ¸ˆã¿ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
        return df

    def create_profit_labels(self, data: pd.DataFrame, horizon: int = 5) -> pd.Series:
        """åç›Šãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ"""
        print(f"ğŸ¯ {horizon}åˆ†å¾Œã®åç›Šãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆä¸­...")
        
        transaction_cost = 0.0012  # 0.12% (0.06% * 2)
        
        # å°†æ¥ãƒªã‚¿ãƒ¼ãƒ³
        future_return = data['close'].pct_change(horizon).shift(-horizon)
        
        # ãƒ­ãƒ³ã‚°ãƒ»ã‚·ãƒ§ãƒ¼ãƒˆåç›Šæ€§
        long_profit = future_return - transaction_cost
        short_profit = -future_return - transaction_cost
        
        # ã©ã¡ã‚‰ã‹ã®æ–¹å‘ã§åˆ©ç›ŠãŒå‡ºã‚‹ã‹ã®åˆ¤å®š
        profitable = ((long_profit > self.profit_threshold) | 
                     (short_profit > self.profit_threshold)).astype(int)
        
        print(f"âœ… åç›Šãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº† (é™½æ€§ç‡: {profitable.mean():.2%})")
        
        return profitable

    def prepare_training_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        print("ğŸ—‚ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
        
        # ç‰¹å¾´é‡åˆ—ã‚’ç‰¹å®š
        feature_cols = [
            'returns', 'log_returns', 'hl_ratio', 'oc_ratio',
            'return_1', 'return_3', 'return_5', 'return_10', 'return_15', 'return_30',
            'vol_5', 'vol_10', 'vol_20', 'vol_30',
            'sma_5', 'sma_10', 'sma_20',
            'price_vs_sma_5', 'price_vs_sma_10', 'price_vs_sma_20',
            'momentum_3', 'momentum_5', 'momentum_10',
            'volume_ma', 'volume_ratio', 'log_volume',
            'rsi', 'bb_position',
            'trend_strength', 'price_above_ma',
            'volume_price_change',
            'high_vol', 'low_vol',
            'hour', 'day_of_week'
        ]
        
        self.feature_names = feature_cols
        print(f"ğŸ“‹ é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡: {len(feature_cols)}å€‹")
        
        # åç›Šãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
        y = self.create_profit_labels(data)
        
        # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        X = data[feature_cols].copy()
        
        # æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã‚’ä½¿ç”¨
        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[valid_mask]
        y = y[valid_mask]
        
        # æ®‹ã‚Šã®NaNã‚’å‡¦ç†
        for col in X.columns:
            if X[col].dtype in ['float64', 'int64']:
                X[col] = X[col].fillna(X[col].median())
            else:
                X[col] = X[col].fillna(0)
        
        print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
        print(f"   ç‰¹å¾´é‡æ•°: {len(X.columns)}")
        print(f"   é™½æ€§ç‡: {y.mean():.2%}")
        
        return X, y

    def train_ensemble_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ¤– ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆä»¥å‰ã®é«˜æ€§èƒ½è¨­å®šã‚’ä½¿ç”¨ï¼‰
        models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=20,
                min_samples_leaf=10,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'lightgbm': lgb.LGBMClassifier(
                n_estimators=300,
                learning_rate=0.05,
                max_depth=6,
                num_leaves=31,
                min_child_samples=20,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1,
                verbose=-1
            ),
            'catboost': cb.CatBoostClassifier(
                iterations=300,
                learning_rate=0.05,
                depth=6,
                class_weights=[1, 5],  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
                random_seed=42,
                verbose=False
            )
        }
        
        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=3)
        cv_results = {}
        
        for name, model in models.items():
            print(f"ğŸ”„ {name}ã‚’è¨“ç·´ä¸­...")
            
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='roc_auc', n_jobs=-1)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´
            model.fit(X, y)
            self.models[name] = model
            
            cv_results[name] = {
                'auc_mean': cv_scores.mean(),
                'auc_std': cv_scores.std()
            }
            
            print(f"   AUC: {cv_results[name]['auc_mean']:.3f}Â±{cv_results[name]['auc_std']:.3f}")
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ
        print("ğŸ”„ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆä¸­...")
        ensemble_models = [(name, model) for name, model in self.models.items()]
        ensemble = VotingClassifier(estimators=ensemble_models, voting='soft')
        ensemble.fit(X, y)
        self.models['ensemble'] = ensemble
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®CVè©•ä¾¡
        ensemble_cv_scores = cross_val_score(ensemble, X, y, cv=tscv, scoring='roc_auc', n_jobs=-1)
        cv_results['ensemble'] = {
            'auc_mean': ensemble_cv_scores.mean(),
            'auc_std': ensemble_cv_scores.std()
        }
        
        print(f"   ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«AUC: {cv_results['ensemble']['auc_mean']:.3f}Â±{cv_results['ensemble']['auc_std']:.3f}")
        
        return cv_results

    def save_models_and_metadata(self, cv_results: Dict, model_dir: str = "models/restored_high_performance"):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        model_path = Path(model_dir)
        model_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        for name, model in self.models.items():
            model_file = model_path / f"{name}_model.pkl"
            joblib.dump(model, model_file)
            print(f"ğŸ’¾ {name}ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜")
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        scaler_file = model_path / "scaler.pkl"
        joblib.dump(self.scaler, scaler_file)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        import json
        from datetime import datetime
        
        metadata = {
            "model_type": "restored_high_performance_ensemble",
            "feature_count": len(self.feature_names),
            "feature_names": self.feature_names,
            "training_date": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "model_version": "3.0_restored",
            "performance": {
                "cv_results": cv_results,
                "best_model": max(cv_results.keys(), key=lambda k: cv_results[k]['auc_mean']),
                "best_auc": max(cv_results[k]['auc_mean'] for k in cv_results.keys())
            },
            "training_config": {
                "profit_threshold": self.profit_threshold,
                "transaction_cost": 0.0012,
                "horizon": 5,
                "features_approach": "proven_35_features"
            }
        }
        
        metadata_file = model_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {metadata_file}")
        
        return metadata

    def run_backtest_simulation(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ç°¡å˜ãªãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print("ğŸ“ˆ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
        
        best_model_name = max(self.models.keys(), 
                             key=lambda k: roc_auc_score(y, self.models[k].predict_proba(X)[:, 1]))
        best_model = self.models[best_model_name]
        
        # äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
        y_proba = best_model.predict_proba(X)[:, 1]
        
        # ç•°ãªã‚‹é–¾å€¤ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        results = {}
        
        for threshold in [0.5, 0.6, 0.7, 0.8, 0.9]:
            signals = (y_proba > threshold)
            
            if signals.sum() > 0:
                signal_accuracy = y[signals].mean()
                signal_count = signals.sum()
                
                # ç°¡å˜ãªåç›Šè¨ˆç®—ï¼ˆæ‰‹æ•°æ–™è€ƒæ…®ï¼‰
                base_return_per_trade = 0.01  # 1%ã®æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³
                net_return = signal_accuracy * base_return_per_trade - 0.0012
                total_return = net_return * signal_count
                
                results[threshold] = {
                    'signal_count': signal_count,
                    'accuracy': signal_accuracy,
                    'net_return_per_trade': net_return,
                    'total_return': total_return
                }
            else:
                results[threshold] = {
                    'signal_count': 0,
                    'accuracy': 0,
                    'net_return_per_trade': 0,
                    'total_return': 0
                }
        
        print("âœ… ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("="*80)
    print("ğŸš€ é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«å¾©å…ƒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ")
    print("="*80)
    
    # å¾©å…ƒã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    restorer = HighPerformanceModelRestoration(profit_threshold=0.005)
    
    # 1. å¸‚å ´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = restorer.load_market_data(symbol="BTCUSDT", start_date="2024-01-01")
    
    if len(data) < 1000:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™")
        return
    
    # 2. å®Ÿè¨¼æ¸ˆã¿ç‰¹å¾´é‡ç”Ÿæˆ
    data_with_features = restorer.create_proven_features(data)
    
    # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X, y = restorer.prepare_training_data(data_with_features)
    
    if len(X) < 100:
        print("âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™")
        return
    
    # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    cv_results = restorer.train_ensemble_models(X, y)
    
    # 5. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    backtest_results = restorer.run_backtest_simulation(X, y)
    
    # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    metadata = restorer.save_models_and_metadata(cv_results)
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“Š å¾©å…ƒã•ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«çµæœ")
    print("="*80)
    
    print(f"\nğŸ”¢ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"   ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
    print(f"   ç‰¹å¾´é‡æ•°: {len(X.columns)}")
    print(f"   é™½æ€§ç‡: {y.mean():.2%}")
    
    print(f"\nğŸ¯ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
    for model_name, scores in cv_results.items():
        status = "ğŸ†" if scores['auc_mean'] == max(s['auc_mean'] for s in cv_results.values()) else "  "
        print(f"   {status} {model_name.upper():15}: AUC = {scores['auc_mean']:.3f}Â±{scores['auc_std']:.3f}")
    
    best_auc = max(cv_results[k]['auc_mean'] for k in cv_results.keys())
    
    if best_auc >= 0.8:
        print(f"\nâœ… ç´ æ™´ã‚‰ã—ã„æ€§èƒ½ (AUC {best_auc:.3f}) - ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™å®Œäº†")
    elif best_auc >= 0.7:
        print(f"\nğŸŸ¨ è‰¯å¥½ãªæ€§èƒ½ (AUC {best_auc:.3f}) - ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã‚’æ¨å¥¨")
    else:
        print(f"\nâŒ æ€§èƒ½ä¸è¶³ (AUC {best_auc:.3f}) - å†æ¤œè¨ãŒå¿…è¦")
    
    print(f"\nğŸ“ˆ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:")
    for threshold, result in backtest_results.items():
        if result['signal_count'] > 0:
            print(f"   é–¾å€¤ {threshold}: {result['signal_count']}å›å–å¼•, "
                  f"ç²¾åº¦ {result['accuracy']:.1%}, "
                  f"ç·ãƒªã‚¿ãƒ¼ãƒ³ {result['total_return']:.2%}")
        else:
            print(f"   é–¾å€¤ {threshold}: å–å¼•ãªã—")
    
    print(f"\nğŸ’¾ ä¿å­˜å ´æ‰€: models/restored_high_performance/")
    
    return restorer, cv_results, backtest_results


if __name__ == "__main__":
    restorer, cv_results, backtest_results = main()