#!/usr/bin/env python3
"""
é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼ˆAUC 0.867ï¼‰26æ¬¡å…ƒå¾©å…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ - é«˜é€Ÿç‰ˆ
- å°ã•ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜é€Ÿè¨“ç·´
- æœ€é‡è¦26ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨
- ONNXå¤‰æ›å¯¾å¿œ
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import duckdb
from typing import Dict, List, Tuple
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ONNXå¤‰æ›ç”¨
try:
    import onnx
    import skl2onnx
    from skl2onnx import convert_sklearn
    from skl2onnx.common.data_types import FloatTensorType
    ONNX_AVAILABLE = True
except ImportError:
    print("âš ï¸ ONNXå¤‰æ›ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¾å­˜é–¢ä¿‚ä¸è¶³ï¼‰")
    ONNX_AVAILABLE = False

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class FastHighPerformanceModel26D:
    """26æ¬¡å…ƒé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ - é«˜é€Ÿç‰ˆ"""
    
    def __init__(self, profit_threshold: float = 0.005):
        self.profit_threshold = profit_threshold
        self.scaler = StandardScaler()
        self.model = None
        
        # æœ€é‡è¦26ç‰¹å¾´é‡
        self.top_26_features = [
            'returns', 'log_returns', 'hl_ratio', 'oc_ratio',
            'return_1', 'return_3', 'return_5', 'return_10', 'return_15', 'return_30',
            'vol_5', 'vol_10', 'vol_20',
            'price_vs_sma_5', 'price_vs_sma_10', 'price_vs_sma_20',
            'rsi', 'bb_position', 'macd_hist',
            'volume_ratio', 'log_volume', 'volume_price_change',
            'momentum_3', 'momentum_5',
            'trend_strength', 'price_above_ma'
        ]
        
        logger.info(f"é«˜é€Ÿ26æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {len(self.top_26_features)}ç‰¹å¾´é‡")
        
    def load_sample_data(self, symbol: str = "BTCUSDT", limit: int = 10000) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
        print(f"ğŸ“Š {symbol}ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ï¼ˆ{limit:,}ä»¶ï¼‰...")
        
        try:
            conn = duckdb.connect("data/historical_data.duckdb", read_only=True)
            
            query = f"""
            SELECT 
                timestamp,
                open,
                high,
                low,
                close,
                volume
            FROM klines_{symbol.lower()}
            WHERE timestamp >= '2024-06-01'
            ORDER BY timestamp DESC
            LIMIT {limit}
            """
            
            data = conn.execute(query).df()
            
            if len(data) == 0:
                print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
                conn.close()
                return self._generate_realistic_data(limit)
            
            data['timestamp'] = pd.to_datetime(data['timestamp'])
            data.set_index('timestamp', inplace=True)
            data = data.sort_index()  # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆ
            
            conn.close()
            print(f"âœ… {len(data):,}ä»¶ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"æœŸé–“: {data.index.min()} ã‹ã‚‰ {data.index.max()}")
            
            return data
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_realistic_data(limit)
    
    def _generate_realistic_data(self, limit: int) -> pd.DataFrame:
        """ç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        print(f"ğŸ“Š ç¾å®Ÿçš„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­ï¼ˆ{limit:,}ä»¶ï¼‰...")
        
        dates = pd.date_range('2024-06-01', periods=limit, freq='5min')
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼å‹•ä½œï¼ˆBTCãƒ™ãƒ¼ã‚¹ï¼‰
        np.random.seed(42)
        base_price = 70000  # 2024å¹´6æœˆã®BTCä¾¡æ ¼
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ + ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ + ãƒã‚¤ã‚¯ãƒ­æ§‹é€ 
        returns = []
        volatility = 0.002  # 0.2%åŸºæœ¬ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        
        for i in range(len(dates)):
            # æ™‚é–“å¸¯åˆ¥ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            hour = dates[i].hour
            if 8 <= hour <= 16:  # ã‚¢ã‚¸ã‚¢ãƒ»æ¬§å·æ™‚é–“
                vol_mult = 1.3
            elif 14 <= hour <= 22:  # æ¬§å·ãƒ»ç±³å›½æ™‚é–“
                vol_mult = 1.5
            else:  # å¤œé–“
                vol_mult = 0.8
            
            # é€±æœ«åŠ¹æœ
            if dates[i].weekday() >= 5:
                vol_mult *= 0.6
            
            current_vol = volatility * vol_mult
            return_val = np.random.normal(0, current_vol)
            
            # è‡ªå·±ç›¸é–¢ï¼ˆãƒªã‚¢ãƒ«ãªä¾¡æ ¼å‹•ä½œï¼‰
            if i > 0:
                return_val += 0.1 * returns[-1]
            
            returns.append(return_val)
        
        # ä¾¡æ ¼ç³»åˆ—ç”Ÿæˆ
        log_prices = np.log(base_price) + np.cumsum(returns)
        prices = np.exp(log_prices)
        
        # OHLCç”Ÿæˆ
        high_factor = 1 + np.abs(np.random.normal(0, 0.001, len(dates)))
        low_factor = 1 - np.abs(np.random.normal(0, 0.001, len(dates)))
        
        data = pd.DataFrame({
            'timestamp': dates,
            'open': prices,
            'high': prices * high_factor,
            'low': prices * low_factor,
            'close': prices,
            'volume': np.random.lognormal(10, 0.8, len(dates))  # ã‚ˆã‚Šç¾å®Ÿçš„ãªãƒœãƒªãƒ¥ãƒ¼ãƒ 
        })
        
        data.set_index('timestamp', inplace=True)
        print(f"âœ… ç¾å®Ÿçš„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(data):,}ä»¶")
        
        return data

    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """26æ¬¡å…ƒç‰¹å¾´é‡ç”Ÿæˆ"""
        print("ğŸ”§ 26æ¬¡å…ƒç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        
        df = data.copy()
        
        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡ (4å€‹)
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['hl_ratio'] = (df['high'] - df['low']) / df['close']
        df['oc_ratio'] = (df['close'] - df['open']) / df['open']
        
        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¿ãƒ¼ãƒ³ (6å€‹)
        for period in [1, 3, 5, 10, 15, 30]:
            df[f'return_{period}'] = df['close'].pct_change(period)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (3å€‹)
        for window in [5, 10, 20]:
            df[f'vol_{window}'] = df['returns'].rolling(window).std()
        
        # ç§»å‹•å¹³å‡æ¯”è¼ƒ (3å€‹)
        for ma in [5, 10, 20]:
            df[f'sma_{ma}'] = df['close'].rolling(ma).mean()
            df[f'price_vs_sma_{ma}'] = (df['close'] - df[f'sma_{ma}']) / df[f'sma_{ma}']
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / (loss + 1e-8)
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½®
        bb_middle = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        bb_upper = bb_middle + (bb_std * 2)
        bb_lower = bb_middle - (bb_std * 2)
        df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower + 1e-8)
        
        # MACD
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡ (3å€‹)
        df['volume_ma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_ma']
        df['log_volume'] = np.log(df['volume'] + 1)
        df['volume_price_change'] = df['volume_ratio'] * abs(df['returns'])
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  (2å€‹)
        df['momentum_3'] = df['close'].pct_change(3)
        df['momentum_5'] = df['close'].pct_change(5)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ (2å€‹)
        df['trend_strength'] = (df['sma_5'] - df['sma_20']) / df['sma_20']
        df['price_above_ma'] = (df['close'] > df['sma_20']).astype(int)
        
        print(f"âœ… 26ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
        return df
    
    def create_labels(self, data: pd.DataFrame) -> pd.Series:
        """åç›Šãƒ©ãƒ™ãƒ«ç”Ÿæˆ"""
        print("ğŸ¯ åç›Šãƒ©ãƒ™ãƒ«ç”Ÿæˆä¸­...")
        
        transaction_cost = 0.0012
        horizons = [5, 10, 15]  # 5,10,15åˆ†å¾Œ
        
        labels = []
        for horizon in horizons:
            future_return = data['close'].pct_change(horizon).shift(-horizon)
            long_profit = future_return - transaction_cost
            short_profit = -future_return - transaction_cost
            profitable = ((long_profit > self.profit_threshold) | 
                         (short_profit > self.profit_threshold)).astype(int)
            labels.append(profitable)
        
        # ä»»æ„ã®æ™‚é–“è»¸ã§åˆ©ç›ŠãŒå‡ºã‚Œã°OK
        combined_labels = pd.concat(labels, axis=1).any(axis=1).astype(int)
        
        print(f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº† (é™½æ€§ç‡: {combined_labels.mean():.2%})")
        return combined_labels

    def prepare_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        print("ğŸ—‚ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        y = self.create_labels(data)
        X = data[self.top_26_features].copy()
        
        # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ã¿
        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[valid_mask]
        y = y[valid_mask]
        
        # æ¬ æå€¤å‡¦ç†
        for col in X.columns:
            if X[col].dtype in ['float64', 'int64']:
                X[col] = X[col].fillna(X[col].median())
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(X):,}ã‚µãƒ³ãƒ—ãƒ«, 26ç‰¹å¾´é‡, é™½æ€§ç‡{y.mean():.2%}")
        return X, y

    def train_model(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """é«˜é€Ÿãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆRandom Forestã®ã¿ï¼‰"""
        print("ğŸ¤– é«˜æ€§èƒ½Random Forestãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        # é«˜æ€§èƒ½è¨­å®šï¼ˆé«˜é€Ÿç‰ˆï¼‰
        self.model = RandomForestClassifier(
            n_estimators=200,    # é«˜é€ŸåŒ–ã®ãŸã‚å‰Šæ¸›
            max_depth=15,        # æ·±ãè¨­å®š
            min_samples_split=3,
            min_samples_leaf=2,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1,
            max_features='sqrt'  # ç‰¹å¾´é‡æ•°ã‚’åˆ¶é™
        )
        
        # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        tscv = TimeSeriesSplit(n_splits=3, test_size=int(len(X)*0.2))
        cv_scores = cross_val_score(self.model, X, y, cv=tscv, scoring='roc_auc', n_jobs=-1)
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.model.fit(X, y)
        
        # ç‰¹å¾´é‡é‡è¦åº¦
        feature_importance = dict(zip(X.columns, self.model.feature_importances_))
        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
        print(f"   AUC: {cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")
        print(f"   ä¸Šä½ç‰¹å¾´é‡: {[f[0] for f in sorted_features[:5]]}")
        
        return {
            'auc_mean': cv_scores.mean(),
            'auc_std': cv_scores.std(),
            'cv_scores': cv_scores.tolist(),
            'feature_importance': feature_importance
        }

    def convert_to_onnx(self, model_dir: Path) -> bool:
        """ONNXå¤‰æ›"""
        if not ONNX_AVAILABLE or not self.model:
            print("âŒ ONNXå¤‰æ›ã‚¹ã‚­ãƒƒãƒ—")
            return False
        
        print("ğŸ”„ ONNXã«å¤‰æ›ä¸­...")
        
        try:
            initial_type = [('float_input', FloatTensorType([None, 26]))]
            onnx_model = convert_sklearn(
                self.model,
                initial_types=initial_type,
                target_opset=14
            )
            
            onnx_path = model_dir / "model.onnx"
            with open(onnx_path, "wb") as f:
                f.write(onnx_model.SerializeToString())
            
            print(f"âœ… ONNXå¤‰æ›å®Œäº†: {onnx_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_model(self, results: Dict, model_dir: str = "models/fast_restored_26d"):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        model_path = Path(model_dir)
        model_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {model_path}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_file = model_path / "model.pkl"
        joblib.dump(self.model, model_file)
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        scaler_file = model_path / "scaler.pkl"
        joblib.dump(self.scaler, scaler_file)
        
        # ONNXå¤‰æ›
        onnx_success = self.convert_to_onnx(model_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        import json
        from datetime import datetime
        
        metadata = {
            "model_type": "fast_restored_high_performance_26d",
            "feature_count": 26,
            "feature_names": self.top_26_features,
            "training_date": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "model_version": "4.0_fast_restored",
            "performance": results,
            "onnx_converted": onnx_success,
            "config": {
                "profit_threshold": self.profit_threshold,
                "transaction_cost": 0.0012,
                "horizons": [5, 10, 15],
                "target_auc": 0.867
            }
        }
        
        metadata_file = model_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ä¿å­˜å®Œäº†: {model_path}")
        return metadata

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("="*70)
    print("ğŸš€ é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«26æ¬¡å…ƒå¾©å…ƒ - é«˜é€Ÿç‰ˆ")
    print("="*70)
    
    # åˆæœŸåŒ–
    model = FastHighPerformanceModel26D(profit_threshold=0.005)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰
    data = model.load_sample_data(limit=10000)
    
    if len(data) < 1000:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return None, None
    
    # ç‰¹å¾´é‡ç”Ÿæˆ
    data_with_features = model.create_features(data)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X, y = model.prepare_data(data_with_features)
    
    if len(X) < 500:
        print("âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        return None, None
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    results = model.train_model(X, y)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    metadata = model.save_model(results)
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*70)
    print("ğŸ“Š 26æ¬¡å…ƒé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«å¾©å…ƒçµæœ")
    print("="*70)
    
    auc = results['auc_mean']
    print(f"\nğŸ¯ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
    print(f"   Random Forest AUC: {auc:.3f}Â±{results['auc_std']:.3f}")
    
    if auc >= 0.8:
        print(f"\nğŸ‰ å„ªç§€ãªæ€§èƒ½ï¼ç›®æ¨™AUC 0.867ã«è¿‘ã„æˆæœ")
        status = "âœ… ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨"
    elif auc >= 0.7:
        print(f"\nâœ… è‰¯å¥½ãªæ€§èƒ½ï¼å®Ÿç”¨å¯èƒ½ãƒ¬ãƒ™ãƒ«")
        status = "âœ… ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½"
    elif auc >= 0.6:
        print(f"\nğŸŸ¨ æ¨™æº–çš„æ€§èƒ½ã€‚ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ ã‚ˆã‚Šæ”¹å–„è¦‹è¾¼ã¿")
        status = "ğŸŸ¨ æ¡ä»¶ä»˜ããƒ‡ãƒ—ãƒ­ã‚¤"
    else:
        print(f"\nâŒ æ€§èƒ½ä¸è¶³ã€‚è¿½åŠ èª¿æ•´ãŒå¿…è¦")
        status = "âŒ è¦æ”¹å–„"
    
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
    print(f"   ç‰¹å¾´é‡æ•°: 26æ¬¡å…ƒ")
    print(f"   é™½æ€§ç‡: {y.mean():.2%}")
    
    print(f"\nğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤çŠ¶æ³: {status}")
    print(f"ğŸ’¾ ä¿å­˜å ´æ‰€: models/fast_restored_26d/")
    
    if metadata.get('onnx_converted'):
        print(f"ğŸ”„ ONNXå¤‰æ›: âœ… å®Œäº†")
    
    print("\nğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. dynamic_trading_coordinator.pyã§ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹å¤‰æ›´")
    print("2. æ–°ã—ã„FeatureAdapter26ã®ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰")
    print("3. ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ")
    
    return model, results

if __name__ == "__main__":
    try:
        model, results = main()
        if model and results:
            print(f"\nâœ… é«˜é€Ÿå¾©å…ƒå®Œäº†! AUC: {results['auc_mean']:.3f}")
        else:
            print("\nâŒ å¾©å…ƒå¤±æ•—")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()