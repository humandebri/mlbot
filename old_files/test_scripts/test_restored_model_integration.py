#!/usr/bin/env python3
"""
é«˜æ€§èƒ½å¾©å…ƒãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ
- æ–°ã—ã„balanced_restored_26dãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ç¢ºèª
- FeatureAdapter26ã¨ã®äº’æ›æ€§ç¢ºèª
- äºˆæ¸¬æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
"""

import sys
import asyncio
from pathlib import Path
import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

async def test_model_integration():
    """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("="*70)
    print("ğŸ§ª é«˜æ€§èƒ½å¾©å…ƒãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("="*70)
    
    try:
        # 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        print("\n1ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªä¸­...")
        
        model_path = Path("models/balanced_restored_26d/model.onnx")
        scaler_path = Path("models/balanced_restored_26d/scaler.pkl")
        metadata_path = Path("models/balanced_restored_26d/metadata.json")
        
        if not model_path.exists():
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
            return False
        
        if not scaler_path.exists():
            print(f"âŒ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {scaler_path}")
            return False
        
        if not metadata_path.exists():
            print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_path}")
            return False
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªå®Œäº†:")
        print(f"   - {model_path} ({model_path.stat().st_size / 1024:.1f} KB)")
        print(f"   - {scaler_path} ({scaler_path.stat().st_size / 1024:.1f} KB)")
        print(f"   - {metadata_path} ({metadata_path.stat().st_size / 1024:.1f} KB)")
        
        # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        print("\n2ï¸âƒ£ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèªä¸­...")
        
        import json
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:")
        print(f"   - ãƒ¢ãƒ‡ãƒ«å‹: {metadata['model_type']}")
        print(f"   - ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {metadata['model_version']}")
        print(f"   - ç‰¹å¾´é‡æ•°: {metadata['feature_count']}")
        print(f"   - AUC: {metadata['performance']['auc_mean']:.3f}")
        print(f"   - ONNXå¤‰æ›: {metadata['onnx_converted']}")
        
        # 3. InferenceEngineåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        print("\n3ï¸âƒ£ InferenceEngineåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ...")
        
        from src.ml_pipeline.inference_engine import InferenceEngine, InferenceConfig
        
        config = InferenceConfig(
            model_path=str(model_path),
            preprocessor_path=str(scaler_path),
            max_inference_time_ms=100.0,
            batch_size=32,
            providers=["CPUExecutionProvider"]
        )
        
        engine = InferenceEngine(config)
        engine.load_model(str(model_path))
        
        print(f"âœ… InferenceEngineåˆæœŸåŒ–æˆåŠŸ")
        print(f"   - ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
        print(f"   - å…¥åŠ›æ¬¡å…ƒ: 26æ¬¡å…ƒæœŸå¾…")
        
        # 4. FeatureAdapterRestored26Dãƒ†ã‚¹ãƒˆ
        print("\n4ï¸âƒ£ FeatureAdapterRestored26Dãƒ†ã‚¹ãƒˆ...")
        
        from src.ml_pipeline.feature_adapter_restored_26d import FeatureAdapterRestored26D
        
        adapter = FeatureAdapterRestored26D()
        
        # ã‚µãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡ï¼ˆå¤šæ¬¡å…ƒï¼‰
        sample_features = {
            'close': 70000.0,
            'open': 69500.0,
            'high': 70200.0,
            'low': 69300.0,
            'volume': 1500000.0,
            'returns': 0.0072,
            'log_returns': 0.0071,
            'hl_ratio': 0.0013,
            'oc_ratio': 0.0072,
            'return_1': 0.0024,
            'return_3': 0.0051,
            'return_5': 0.0089,
            'return_10': 0.0156,
            'vol_5': 0.0145,
            'vol_10': 0.0142,
            'vol_20': 0.0138,
            'rsi': 65.4,
            'bb_position': 0.67,
            'volume_ratio': 1.34,
            'momentum_3': 0.0051,
            'momentum_5': 0.0089,
            # Extra features (should be ignored)
            'extra_feature_1': 123.45,
            'extra_feature_2': 678.90,
            'irrelevant_data': 'ignored'
        }
        
        adapted_features = adapter.adapt(sample_features)
        stats = adapter.get_adaptation_stats(sample_features)
        
        print(f"âœ… FeatureAdapterRestored26Då‹•ä½œç¢ºèª:")
        print(f"   - å…¥åŠ›ç‰¹å¾´é‡æ•°: {len(sample_features)}")
        print(f"   - å‡ºåŠ›æ¬¡å…ƒ: {adapted_features.shape}")
        print(f"   - ãƒãƒƒãƒç‡: {stats['match_rate']:.1%}")
        print(f"   - é©åˆç‰¹å¾´é‡: {stats['matched_features']}/{stats['target_features']}")
        print(f"   - å°å‡ºç‰¹å¾´é‡: {stats.get('derived_features', 0)}")
        
        if adapted_features.shape[0] != 26:
            print(f"âŒ å‡ºåŠ›æ¬¡å…ƒã‚¨ãƒ©ãƒ¼: æœŸå¾…26æ¬¡å…ƒã€å®Ÿéš›{adapted_features.shape[0]}æ¬¡å…ƒ")
            return False
        
        # 5. äºˆæ¸¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        print("\n5ï¸âƒ£ äºˆæ¸¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆ...")
        
        # FeatureAdapterRestored26Dã§å¤‰æ›ã—ãŸç‰¹å¾´é‡ã‚’ä½¿ç”¨
        feature_array = adapted_features.reshape(1, -1).astype(np.float32)
        
        prediction = engine.predict(feature_array)
        
        print(f"âœ… äºˆæ¸¬å®Ÿè¡ŒæˆåŠŸ:")
        print(f"   - å…¥åŠ›å½¢çŠ¶: {feature_array.shape}")
        print(f"   - äºˆæ¸¬å‹: {type(prediction)}")
        
        # InferenceEngineã¯è¾æ›¸å½¢å¼ã§çµæœã‚’è¿”ã™
        if isinstance(prediction, dict):
            pred_array = prediction.get('predictions', [])
            confidence_scores = prediction.get('confidence_scores', [])
            inference_time = prediction.get('inference_time_ms', 0)
            
            if len(pred_array) > 0:
                pred_value = float(pred_array[0])
                confidence = float(confidence_scores[0]) if len(confidence_scores) > 0 else 0.5
                
                print(f"   - äºˆæ¸¬å€¤: {pred_value:.4f}")
                print(f"   - ä¿¡é ¼åº¦: {confidence:.1%}")
                print(f"   - æ¨è«–æ™‚é–“: {inference_time:.2f}ms")
                
                if confidence > 0.8:
                    print(f"   ğŸ¯ é«˜ä¿¡é ¼åº¦ã‚·ã‚°ãƒŠãƒ« (>80%)")
                elif confidence > 0.6:
                    print(f"   ğŸ“Š ä¸­ä¿¡é ¼åº¦ã‚·ã‚°ãƒŠãƒ« (>60%)")
                else:
                    print(f"   ğŸ“‹ ä½ä¿¡é ¼åº¦ (<60%)")
            else:
                print("   âŒ äºˆæ¸¬å€¤ãŒç©ºã§ã™")
        else:
            print(f"   âš ï¸ äºˆæœŸã—ãªã„äºˆæ¸¬å½¢å¼: {prediction}")
        
        # 6. çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆDynamicTradingCoordinatoré¢¨ï¼‰
        print("\n6ï¸âƒ£ çµ±åˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³...")
        
        # è¤‡æ•°ã®ç‰¹å¾´é‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ†ã‚¹ãƒˆ
        test_cases = [
            {"close": 70000, "returns": 0.005, "vol_20": 0.015, "rsi": 70},  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
            {"close": 69000, "returns": -0.003, "vol_20": 0.020, "rsi": 30}, # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
            {"close": 69500, "returns": 0.001, "vol_20": 0.010, "rsi": 50},  # ãƒ¬ãƒ³ã‚¸
        ]
        
        scenarios = ["ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰", "ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰", "ãƒ¬ãƒ³ã‚¸ç›¸å ´"]
        
        for i, (test_case, scenario) in enumerate(zip(test_cases, scenarios)):
            # åŸºæœ¬ç‰¹å¾´é‡ã‹ã‚‰å®Œå…¨ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰
            full_features = {**sample_features, **test_case}
            
            adapted = adapter.adapt(full_features)
            input_array = adapted.reshape(1, -1).astype(np.float32)
            pred = engine.predict(input_array)
            
            if isinstance(pred, dict):
                pred_array = pred.get('predictions', [])
                confidence_scores = pred.get('confidence_scores', [])
                pred_val = float(pred_array[0]) if len(pred_array) > 0 else 0.0
                confidence = float(confidence_scores[0]) if len(confidence_scores) > 0 else 0.5
            else:
                pred_val = pred[0] if isinstance(pred, (list, np.ndarray)) else pred
                confidence = 1 / (1 + np.exp(-5 * (pred_val - 0.5)))
            
            print(f"   {i+1}. {scenario}: äºˆæ¸¬={pred_val:.3f}, ä¿¡é ¼åº¦={confidence:.1%}")
        
        # 7. æœ€çµ‚ç¢ºèª
        print("\n7ï¸âƒ£ æœ€çµ‚ç¢ºèª...")
        
        success_checks = [
            model_path.exists(),
            metadata['feature_count'] == 26,
            metadata['onnx_converted'] == True,
            adapted_features.shape[0] == 26,
            stats['match_rate'] > 0.5,
            prediction is not None
        ]
        
        if all(success_checks):
            print("âœ… å…¨ãƒ†ã‚¹ãƒˆPASSï¼é«˜æ€§èƒ½å¾©å…ƒãƒ¢ãƒ‡ãƒ«çµ±åˆå®Œäº†")
            print(f"\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœŸå¾…å€¤:")
            print(f"   - AUC: {metadata['performance']['auc_mean']:.3f} (ç›®æ¨™0.867ã®{metadata['performance']['auc_mean']/0.867:.1%})")
            print(f"   - ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ æ¯”: +{(metadata['performance']['auc_mean']-0.700)/0.700:.1%}æ”¹å–„")
            print(f"   - ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™: âœ… å®Œäº†")
            
            return True
        else:
            print("âŒ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—")
            return False
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        result = asyncio.run(test_model_integration())
        
        if result:
            print("\n" + "="*70)
            print("ğŸ‰ é«˜æ€§èƒ½å¾©å…ƒãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
            print("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print("1. EC2ã§ã®å‹•ä½œç¢ºèª")
            print("2. ãƒ©ã‚¤ãƒ–å–å¼•ã§ã®æ€§èƒ½æ¤œè¨¼")
            print("3. ä¿¡é ¼åº¦é–¾å€¤ã®æœ€é©åŒ–ï¼ˆæ¨å¥¨75-85%ï¼‰")
            print("="*70)
        else:
            print("\nâŒ çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—ã€‚å•é¡Œã‚’ä¿®æ­£ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆä¸­æ–­")
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()