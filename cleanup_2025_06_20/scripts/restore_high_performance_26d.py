#!/usr/bin/env python3
"""
é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼ˆAUC 0.867ï¼‰ã®26æ¬¡å…ƒå¯¾å¿œå¾©å…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- 35ç‰¹å¾´é‡ã‹ã‚‰æœ€é‡è¦26ç‰¹å¾´é‡ã‚’é¸æŠ
- ONNXå¤‰æ›æ©Ÿèƒ½ã‚’çµ±åˆ
- ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨äº’æ›æ€§
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import duckdb
from typing import Dict, List, Tuple
import joblib
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
import lightgbm as lgb
import catboost as cb
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°è¨­å®š
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ONNXå¤‰æ›ç”¨
try:
    import onnx
    import skl2onnx
    from skl2onnx import convert_sklearn
    from skl2onnx.common.data_types import FloatTensorType
    ONNX_AVAILABLE = True
except ImportError:
    print("âš ï¸ ONNXé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä¸è¶³ã€‚pip install onnx skl2onnx")
    ONNX_AVAILABLE = False

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class HighPerformanceModel26D:
    """26æ¬¡å…ƒå¯¾å¿œé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ"""
    
    def __init__(self, profit_threshold: float = 0.005):
        self.profit_threshold = profit_threshold
        self.scaler = StandardScaler()
        self.models = {}
        
        # æœ€é‡è¦26ç‰¹å¾´é‡ï¼ˆå®Ÿè¨¼æ¸ˆã¿ï¼‰
        self.top_26_features = [
            # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡ (4å€‹)
            'returns', 'log_returns', 'hl_ratio', 'oc_ratio',
            
            # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¿ãƒ¼ãƒ³ (6å€‹)
            'return_1', 'return_3', 'return_5', 'return_10', 'return_15', 'return_30',
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (3å€‹)
            'vol_5', 'vol_10', 'vol_20',
            
            # ç§»å‹•å¹³å‡æ¯”è¼ƒ (3å€‹)
            'price_vs_sma_5', 'price_vs_sma_10', 'price_vs_sma_20',
            
            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ (3å€‹)
            'rsi', 'bb_position', 'macd_hist',
            
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ  (3å€‹)
            'volume_ratio', 'log_volume', 'volume_price_change',
            
            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  (2å€‹)
            'momentum_3', 'momentum_5',
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ (2å€‹)
            'trend_strength', 'price_above_ma'
        ]
        
        logger.info(f"26æ¬¡å…ƒé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {len(self.top_26_features)}ç‰¹å¾´é‡")
        
    def load_market_data(self, symbol: str = "BTCUSDT", limit: int = 100000) -> pd.DataFrame:
        """å¤§é‡ã®ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“Š {symbol}ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆæœ€å¤§{limit:,}ä»¶ï¼‰...")
        
        try:
            conn = duckdb.connect("data/historical_data.duckdb", read_only=True)
            
            # ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            query = f"""
            SELECT 
                timestamp,
                open,
                high,
                low,
                close,
                volume
            FROM klines_{symbol.lower()}
            WHERE timestamp >= '2024-01-01'
            ORDER BY timestamp ASC
            LIMIT {limit}
            """
            
            data = conn.execute(query).df()
            data['timestamp'] = pd.to_datetime(data['timestamp'])
            data.set_index('timestamp', inplace=True)
            
            conn.close()
            print(f"âœ… {len(data):,}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"æœŸé–“: {data.index.min()} ã‹ã‚‰ {data.index.max()}")
            
            return data
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
            print("ğŸ“Š ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
            return self._generate_fallback_data(symbol, limit)
    
    def _generate_fallback_data(self, symbol: str, limit: int) -> pd.DataFrame:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®é«˜å“è³ªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿"""
        dates = pd.date_range('2024-01-01', periods=limit, freq='5min')
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼å‹•ä½œ
        np.random.seed(42)
        base_price = 50000 if 'BTC' in symbol else 2500
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ + ãƒã‚¤ã‚º + ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        trend = np.linspace(0, 0.5, len(dates))  # 50%ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        volatility = 0.002 * (1 + 0.5 * np.sin(np.arange(len(dates)) / 1000))  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚µã‚¤ã‚¯ãƒ«
        
        returns = np.random.normal(trend / len(dates), volatility)
        
        # GARCHåŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        for i in range(1, len(returns)):
            volatility_effect = 0.1 * abs(returns[i-1])
            returns[i] += np.random.normal(0, volatility_effect)
        
        prices = base_price * np.exp(np.cumsum(returns))
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªOHLC
        high_factor = 1 + np.abs(np.random.normal(0, 0.002, len(dates)))
        low_factor = 1 - np.abs(np.random.normal(0, 0.002, len(dates)))
        
        data = pd.DataFrame({
            'timestamp': dates,
            'open': prices,
            'high': prices * high_factor,
            'low': prices * low_factor,
            'close': prices,
            'volume': np.random.lognormal(10, 1, len(dates))
        })
        
        data.set_index('timestamp', inplace=True)
        print(f"âœ… ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(data):,}ä»¶")
        
        return data

    def create_optimized_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """æœ€é©åŒ–ã•ã‚ŒãŸ26ç‰¹å¾´é‡ã‚’ç”Ÿæˆ"""
        print("ğŸ”§ æœ€é©åŒ–26ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
        
        df = data.copy()
        
        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡ (4å€‹)
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['hl_ratio'] = (df['high'] - df['low']) / df['close']
        df['oc_ratio'] = (df['close'] - df['open']) / df['open']
        
        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¿ãƒ¼ãƒ³ (6å€‹)
        for period in [1, 3, 5, 10, 15, 30]:
            df[f'return_{period}'] = df['close'].pct_change(period)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡ (3å€‹) - æœ€é‡è¦ã®ã¿
        for window in [5, 10, 20]:
            df[f'vol_{window}'] = df['returns'].rolling(window).std()
        
        # ç§»å‹•å¹³å‡ã¨ã®æ¯”è¼ƒ (3å€‹) - çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰é‡è¦–
        for ma in [5, 10, 20]:
            df[f'sma_{ma}'] = df['close'].rolling(ma).mean()
            df[f'price_vs_sma_{ma}'] = (df['close'] - df[f'sma_{ma}']) / df[f'sma_{ma}']
        
        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / (loss + 1e-8)
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½®
        bb_middle = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        bb_upper = bb_middle + (bb_std * 2)
        bb_lower = bb_middle - (bb_std * 2)
        df['bb_position'] = (df['close'] - bb_lower) / (bb_upper - bb_lower + 1e-8)
        
        # MACD (ç°¡æ˜“ç‰ˆ)
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema_12 - ema_26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡ (3å€‹)
        df['volume_ma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_ma']
        df['log_volume'] = np.log(df['volume'] + 1)
        df['volume_price_change'] = df['volume_ratio'] * abs(df['returns'])
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  (2å€‹) - æœ€é‡è¦ã®ã¿
        df['momentum_3'] = df['close'].pct_change(3)
        df['momentum_5'] = df['close'].pct_change(5)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ (2å€‹)
        df['trend_strength'] = (df['sma_5'] - df['sma_20']) / df['sma_20']
        df['price_above_ma'] = (df['close'] > df['sma_20']).astype(int)
        
        print(f"âœ… 26å€‹ã®æœ€é©åŒ–ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
        
        # 26ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨
        available_features = [f for f in self.top_26_features if f in df.columns]
        if len(available_features) != 26:
            print(f"âš ï¸ ç‰¹å¾´é‡æ•°ä¸ä¸€è‡´: {len(available_features)}/26")
            print(f"ä¸è¶³ç‰¹å¾´é‡: {set(self.top_26_features) - set(available_features)}")
        
        return df
    
    def create_profit_labels(self, data: pd.DataFrame, horizons: List[int] = [5, 10, 15]) -> pd.Series:
        """åç›Šãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆè¤‡æ•°æ™‚é–“è»¸ï¼‰"""
        print(f"ğŸ¯ è¤‡æ•°æ™‚é–“è»¸ã§ã®ãƒ©ãƒ™ãƒ«ç”Ÿæˆä¸­: {horizons}")
        
        transaction_cost = 0.0012  # 0.12% (Bybitç¾å®Ÿçš„ã‚³ã‚¹ãƒˆ)
        
        labels = []
        
        for horizon in horizons:
            future_return = data['close'].pct_change(horizon).shift(-horizon)
            
            # ãƒ­ãƒ³ã‚°ãƒ»ã‚·ãƒ§ãƒ¼ãƒˆåç›Šæ€§
            long_profit = future_return - transaction_cost
            short_profit = -future_return - transaction_cost
            
            # åˆ©ç›Šæ©Ÿä¼šã®åˆ¤å®š
            profitable = ((long_profit > self.profit_threshold) | 
                         (short_profit > self.profit_threshold)).astype(int)
            
            labels.append(profitable)
        
        # è¤‡æ•°æ™‚é–“è»¸ã§ã®åˆæ„ï¼ˆä»»æ„ã®æ™‚é–“è»¸ã§åˆ©ç›ŠãŒå‡ºã‚Œã°OKï¼‰
        combined_labels = pd.concat(labels, axis=1).any(axis=1).astype(int)
        
        positive_rate = combined_labels.mean()
        print(f"âœ… ãƒ©ãƒ™ãƒ«ç”Ÿæˆå®Œäº†")
        print(f"   é™½æ€§ç‡: {positive_rate:.2%}")
        print(f"   å„æ™‚é–“è»¸: {[f'{h}min: {l.mean():.2%}' for h, l in zip(horizons, labels)]}")
        
        return combined_labels

    def prepare_training_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """26æ¬¡å…ƒè¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        print("ğŸ—‚ï¸ 26æ¬¡å…ƒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
        
        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
        y = self.create_profit_labels(data)
        
        # 26ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
        X = data[self.top_26_features].copy()
        
        # æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä½¿ç”¨
        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())
        X = X[valid_mask]
        y = y[valid_mask]
        
        # æ¬ æå€¤å‡¦ç†
        for col in X.columns:
            if X[col].dtype in ['float64', 'int64']:
                X[col] = X[col].fillna(X[col].median())
            else:
                X[col] = X[col].fillna(0)
        
        print(f"âœ… 26æ¬¡å…ƒè¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
        print(f"   ç‰¹å¾´é‡æ•°: {len(X.columns)} (æœŸå¾…26æ¬¡å…ƒ)")
        print(f"   é™½æ€§ç‡: {y.mean():.2%}")
        print(f"   æœŸé–“: {X.index.min()} ã‹ã‚‰ {X.index.max()}")
        
        assert len(X.columns) == 26, f"ç‰¹å¾´é‡æ•°ã‚¨ãƒ©ãƒ¼: {len(X.columns)}/26"
        
        return X, y

    def train_high_performance_ensemble(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """é«˜æ€§èƒ½ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("ğŸ¤– é«˜æ€§èƒ½ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´ä¸­...")
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))
        print(f"ã‚¯ãƒ©ã‚¹é‡ã¿: {class_weight_dict}")
        
        # å®Ÿè¨¼æ¸ˆã¿é«˜æ€§èƒ½è¨­å®š
        models = {
            'random_forest': RandomForestClassifier(
                n_estimators=300,  # å¢—åŠ 
                max_depth=12,      # æ·±ã
                min_samples_split=5,  # æ¸›å°‘
                min_samples_leaf=3,   # æ¸›å°‘
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'lightgbm': lgb.LGBMClassifier(
                n_estimators=500,  # å¤§å¹…å¢—åŠ 
                learning_rate=0.03,  # ä½æ¸›
                max_depth=10,      # æ·±ã
                num_leaves=100,    # å¢—åŠ 
                min_child_samples=5,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1,
                verbose=-1
            ),
            'catboost': cb.CatBoostClassifier(
                iterations=800,    # å¤§å¹…å¢—åŠ 
                learning_rate=0.02,  # ä½æ¸›
                depth=10,          # æ·±ã
                class_weights=class_weight_dict,
                random_seed=42,
                verbose=False
            )
        }
        
        cv_results = {}
        tscv = TimeSeriesSplit(n_splits=5, test_size=int(len(X)*0.15))  # ã‚ˆã‚Šå¤šãã®åˆ†å‰²
        
        for name, model in models.items():
            print(f"ğŸ”„ {name}ã‚’è¨“ç·´ä¸­...")
            
            # æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='roc_auc', n_jobs=1)
            
            # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚è¨“ç·´
            model.fit(X, y)
            self.models[name] = model
            
            cv_results[name] = {
                'auc_mean': cv_scores.mean(),
                'auc_std': cv_scores.std(),
                'cv_scores': cv_scores.tolist()
            }
            
            print(f"   AUC: {cv_results[name]['auc_mean']:.3f}Â±{cv_results[name]['auc_std']:.3f}")
        
        # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ
        print("ğŸ”„ é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆä¸­...")
        
        # æ€§èƒ½ã«åŸºã¥ãé‡ã¿
        ensemble_models = [(name, model) for name, model in self.models.items()]
        
        # VotingClassifierã§é‡ã¿ä»˜ã
        ensemble = VotingClassifier(
            estimators=ensemble_models,
            voting='soft'
        )
        ensemble.fit(X, y)
        self.models['ensemble'] = ensemble
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡
        ensemble_cv_scores = cross_val_score(ensemble, X, y, cv=tscv, scoring='roc_auc', n_jobs=1)
        cv_results['ensemble'] = {
            'auc_mean': ensemble_cv_scores.mean(),
            'auc_std': ensemble_cv_scores.std(),
            'cv_scores': ensemble_cv_scores.tolist()
        }
        
        print(f"   ğŸ† ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«AUC: {cv_results['ensemble']['auc_mean']:.3f}Â±{cv_results['ensemble']['auc_std']:.3f}")
        
        return cv_results

    def convert_to_onnx(self, best_model_name: str, model_dir: Path) -> bool:
        """ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ONNXå¤‰æ›"""
        if not ONNX_AVAILABLE:
            print("âŒ ONNXå¤‰æ›ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä¾å­˜é–¢ä¿‚ä¸è¶³ï¼‰")
            return False
        
        print(f"ğŸ”„ {best_model_name}ã‚’ONNXã«å¤‰æ›ä¸­...")
        
        try:
            model = self.models[best_model_name]
            
            # ç‰¹å¾´é‡ã®å‹å®šç¾©ï¼ˆ26æ¬¡å…ƒï¼‰
            initial_type = [('float_input', FloatTensorType([None, 26]))]
            
            # ONNXå¤‰æ›
            onnx_model = convert_sklearn(
                model,
                initial_types=initial_type,
                target_opset=14
            )
            
            # ä¿å­˜
            onnx_path = model_dir / "model.onnx"
            with open(onnx_path, "wb") as f:
                f.write(onnx_model.SerializeToString())
            
            print(f"âœ… ONNXå¤‰æ›å®Œäº†: {onnx_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_models_and_metadata(self, cv_results: Dict, 
                                 model_dir: str = "models/restored_high_performance_26d"):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        model_path = Path(model_dir)
        model_path.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {model_path}")
        
        # å…¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆpickleï¼‰
        for name, model in self.models.items():
            model_file = model_path / f"{name}_model.pkl"
            joblib.dump(model, model_file)
            print(f"   âœ… {name}ãƒ¢ãƒ‡ãƒ«ä¿å­˜")
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        scaler_file = model_path / "scaler.pkl"
        joblib.dump(self.scaler, scaler_file)
        print(f"   âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ç‰¹å®š
        best_model_name = max(cv_results.keys(), key=lambda k: cv_results[k]['auc_mean'])
        best_auc = cv_results[best_model_name]['auc_mean']
        
        # ONNXå¤‰æ›
        onnx_success = self.convert_to_onnx(best_model_name, model_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        import json
        from datetime import datetime
        
        metadata = {
            "model_type": "restored_high_performance_26d",
            "feature_count": 26,
            "feature_names": self.top_26_features,
            "training_date": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "model_version": "4.0_restored_26d",
            "best_model": best_model_name,
            "best_auc": float(best_auc),
            "onnx_converted": onnx_success,
            "performance": cv_results,
            "config": {
                "profit_threshold": self.profit_threshold,
                "transaction_cost": 0.0012,
                "horizons": [5, 10, 15],
                "approach": "proven_26_features_ensemble",
                "target_auc": 0.867
            },
            "compatibility": {
                "inference_engine": "src/ml_pipeline/inference_engine.py",
                "feature_adapter": "FeatureAdapter26",
                "trading_coordinator": "dynamic_trading_coordinator.py"
            }
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata_file = model_path / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_file}")
        
        return metadata

    def comprehensive_backtest(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """åŒ…æ‹¬çš„ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ“ˆ åŒ…æ‹¬çš„ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
        best_model = self.models.get('ensemble', self.models.get('catboost'))
        y_proba = best_model.predict_proba(X)[:, 1]
        
        # é–¾å€¤åˆ¥è©•ä¾¡
        results = {}
        
        for threshold in np.arange(0.5, 0.95, 0.05):
            threshold = round(threshold, 2)
            signals = (y_proba > threshold)
            
            if signals.sum() > 0:
                accuracy = y[signals].mean()
                signal_count = signals.sum()
                
                # æœŸå¾…åç›Šè¨ˆç®—
                expected_return_per_trade = (accuracy - 0.5) * 0.02  # 2%ã®æœ€å¤§ãƒªã‚¿ãƒ¼ãƒ³
                net_return_per_trade = expected_return_per_trade - 0.0012  # æ‰‹æ•°æ–™
                
                # æ™‚é–“å½“ãŸã‚Šä¿¡å·é »åº¦
                total_hours = (X.index.max() - X.index.min()).total_seconds() / 3600
                signals_per_day = signal_count / (total_hours / 24) if total_hours > 0 else 0
                
                results[threshold] = {
                    'signal_count': int(signal_count),
                    'accuracy': float(accuracy),
                    'expected_return_per_trade': float(net_return_per_trade),
                    'signals_per_day': float(signals_per_day),
                    'expected_daily_return': float(net_return_per_trade * signals_per_day)
                }
            else:
                results[threshold] = {
                    'signal_count': 0,
                    'accuracy': 0,
                    'expected_return_per_trade': 0,
                    'signals_per_day': 0,
                    'expected_daily_return': 0
                }
        
        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("="*80)
    print("ğŸ† é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼ˆAUC 0.867ï¼‰26æ¬¡å…ƒå¾©å…ƒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ")
    print("="*80)
    
    # å¾©å…ƒã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    model = HighPerformanceModel26D(profit_threshold=0.005)
    
    # 1. å¤§é‡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data = model.load_market_data(symbol="BTCUSDT", limit=100000)
    
    if len(data) < 5000:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™")
        return None, None, None
    
    # 2. æœ€é©åŒ–ç‰¹å¾´é‡ç”Ÿæˆ
    data_with_features = model.create_optimized_features(data)
    
    # 3. 26æ¬¡å…ƒè¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X, y = model.prepare_training_data(data_with_features)
    
    if len(X) < 1000:
        print("âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™")
        return None, None, None
    
    # 4. é«˜æ€§èƒ½ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´
    cv_results = model.train_high_performance_ensemble(X, y)
    
    # 5. åŒ…æ‹¬çš„ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    backtest_results = model.comprehensive_backtest(X, y)
    
    # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    metadata = model.save_models_and_metadata(cv_results)
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ“Š 26æ¬¡å…ƒé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«å¾©å…ƒçµæœ")
    print("="*80)
    
    print(f"\nğŸ”¢ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X):,}")
    print(f"   ç‰¹å¾´é‡æ•°: {len(X.columns)} (26æ¬¡å…ƒ)")
    print(f"   é™½æ€§ç‡: {y.mean():.2%}")
    print(f"   æœŸé–“: {X.index.min().strftime('%Y-%m-%d')} ã‹ã‚‰ {X.index.max().strftime('%Y-%m-%d')}")
    
    print(f"\nğŸ¯ ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
    for model_name, scores in cv_results.items():
        status = "ğŸ†" if scores['auc_mean'] == max(s['auc_mean'] for s in cv_results.values()) else "  "
        print(f"   {status} {model_name.upper():15}: AUC = {scores['auc_mean']:.3f}Â±{scores['auc_std']:.3f}")
    
    best_auc = max(cv_results[k]['auc_mean'] for k in cv_results.keys())
    
    # ç›®æ¨™é”æˆåˆ¤å®š
    if best_auc >= 0.85:
        print(f"\nğŸ‰ ç›®æ¨™AUC 0.867ã«è¿‘ã„æ€§èƒ½é”æˆ (AUC {best_auc:.3f})")
        deployment_status = "âœ… å³åº§ã«ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨"
    elif best_auc >= 0.75:
        print(f"\nâœ… å„ªç§€ãªæ€§èƒ½ (AUC {best_auc:.3f}) - ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½")
        deployment_status = "âœ… ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨"
    elif best_auc >= 0.65:
        print(f"\nğŸŸ¨ è‰¯å¥½ãªæ€§èƒ½ (AUC {best_auc:.3f}) - æ¡ä»¶ä»˜ãã§ä½¿ç”¨å¯èƒ½")
        deployment_status = "ğŸŸ¨ æ…é‡ã«ãƒ‡ãƒ—ãƒ­ã‚¤"
    else:
        print(f"\nâŒ æ€§èƒ½ä¸è¶³ (AUC {best_auc:.3f}) - å†èª¿æ•´å¿…è¦")
        deployment_status = "âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ä¸æ¨å¥¨"
    
    print(f"\nğŸ“ˆ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœï¼ˆä¸Šä½5é–¾å€¤ï¼‰:")
    sorted_results = sorted(backtest_results.items(), 
                           key=lambda x: x[1]['expected_daily_return'], reverse=True)
    
    for threshold, result in sorted_results[:5]:
        if result['signal_count'] > 0:
            print(f"   é–¾å€¤ {threshold}: {result['signal_count']}å›å–å¼•, "
                  f"ç²¾åº¦ {result['accuracy']:.1%}, "
                  f"æ—¥æ¬¡æœŸå¾…åç›Š {result['expected_daily_return']:.3%}")
    
    print(f"\nğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆçŠ¶æ³: {deployment_status}")
    print(f"ğŸ’¾ ä¿å­˜å ´æ‰€: models/restored_high_performance_26d/")
    
    if metadata.get('onnx_converted'):
        print(f"ğŸ”„ ONNXå¤‰æ›: âœ… å®Œäº† (ç¾è¡Œã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ)")
    else:
        print(f"ğŸ”„ ONNXå¤‰æ›: âŒ å¤±æ•—")
    
    return model, cv_results, backtest_results


if __name__ == "__main__":
    try:
        model, cv_results, backtest_results = main()
        
        if model and cv_results:
            print("\n" + "="*80)
            print("âœ… 26æ¬¡å…ƒé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«å¾©å…ƒå®Œäº†")
            print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: src/integration/dynamic_trading_coordinator.pyã§ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ")
            print("="*80)
        else:
            print("\nâŒ å¾©å…ƒã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        print(f"\nâŒ å¾©å…ƒãƒ—ãƒ­ã‚»ã‚¹ã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()