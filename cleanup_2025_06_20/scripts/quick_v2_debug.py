#!/usr/bin/env python3
"""
v2.0ãƒ¢ãƒ‡ãƒ«ã®å•é¡Œã‚’è¨ºæ–­ã™ã‚‹ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
"""
import os
import sys
import onnxruntime as ort
import numpy as np
import json
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

def test_v2_model():
    """v2.0ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¦AUC 0.5ã®åŸå› ã‚’ç‰¹å®š"""
    
    model_path = Path(__file__).parent.parent / "models" / "v2.0" / "model.onnx"
    metadata_path = Path(__file__).parent.parent / "models" / "v2.0" / "metadata.json"
    
    if not model_path.exists():
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        return
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    with open(metadata_path) as f:
        metadata = json.load(f)
    
    print(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
    print(f"  ã‚¿ã‚¤ãƒ—: {metadata.get('model_type', 'unknown')}")
    print(f"  ç‰¹å¾´é‡æ•°: {metadata.get('feature_count', 0)}")
    print(f"  è¨“ç·´æ—¥æ™‚: {metadata.get('training_date', 'unknown')}")
    
    # ONNXãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    try:
        session = ort.InferenceSession(str(model_path))
        input_names = [input.name for input in session.get_inputs()]
        output_names = [output.name for output in session.get_outputs()]
        
        print(f"\nONNXãƒ¢ãƒ‡ãƒ«æƒ…å ±:")
        print(f"  å…¥åŠ›å: {input_names}")
        print(f"  å‡ºåŠ›å: {output_names}")
        
        # å…¥åŠ›ã®å½¢çŠ¶ã‚’ç¢ºèª
        for inp in session.get_inputs():
            print(f"  å…¥åŠ›å½¢çŠ¶ '{inp.name}': {inp.shape}")
        
        # å‡ºåŠ›ã®å½¢çŠ¶ã‚’ç¢ºèª
        for out in session.get_outputs():
            print(f"  å‡ºåŠ›å½¢çŠ¶ '{out.name}': {out.shape}")
        
    except Exception as e:
        print(f"ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
        return
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆ156ç‰¹å¾´é‡ï¼‰
    feature_count = metadata.get('feature_count', 156)
    test_samples = 100
    
    # æ­£å¸¸ãªç¯„å›²ã®å€¤ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    np.random.seed(42)
    test_data = np.random.randn(test_samples, feature_count).astype(np.float32)
    
    print(f"\näºˆæ¸¬ãƒ†ã‚¹ãƒˆ:")
    print(f"  ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {test_samples}")
    print(f"  ç‰¹å¾´é‡æ•°: {feature_count}")
    print(f"  ãƒ‡ãƒ¼ã‚¿ç¯„å›²: [{test_data.min():.4f}, {test_data.max():.4f}]")
    
    try:
        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = session.run(output_names, {input_names[0]: test_data})[0]
        
        print(f"\näºˆæ¸¬çµæœ:")
        print(f"  äºˆæ¸¬å€¤ã®å½¢çŠ¶: {predictions.shape}")
        print(f"  äºˆæ¸¬å€¤ã®ç¯„å›²: [{predictions.min():.6f}, {predictions.max():.6f}]")
        print(f"  äºˆæ¸¬å€¤ã®å¹³å‡: {predictions.mean():.6f}")
        print(f"  äºˆæ¸¬å€¤ã®æ¨™æº–åå·®: {predictions.std():.6f}")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªäºˆæ¸¬å€¤æ•°: {len(np.unique(predictions))}")
        
        # åˆ†å¸ƒã‚’ç¢ºèª
        unique_values = np.unique(predictions)
        if len(unique_values) <= 10:
            print(f"  å…¨ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {unique_values}")
        
        # ã™ã¹ã¦ã®äºˆæ¸¬ãŒåŒã˜å€¤ã‹ãƒã‚§ãƒƒã‚¯
        if len(unique_values) == 1:
            print(f"  âš ï¸  è­¦å‘Š: ã™ã¹ã¦ã®äºˆæ¸¬å€¤ãŒåŒã˜ ({unique_values[0]:.6f})")
            print("  ã“ã‚ŒãŒAUC 0.5000ã®åŸå› ã§ã™")
        
        # äºˆæ¸¬å€¤ã®è©³ç´°çµ±è¨ˆ
        print(f"\nè©³ç´°çµ±è¨ˆ:")
        print(f"  25%åˆ†ä½: {np.percentile(predictions, 25):.6f}")
        print(f"  50%åˆ†ä½ (ä¸­å¤®å€¤): {np.percentile(predictions, 50):.6f}")
        print(f"  75%åˆ†ä½: {np.percentile(predictions, 75):.6f}")
        
    except Exception as e:
        print(f"äºˆæ¸¬ã®å®Ÿè¡Œã«å¤±æ•—: {e}")
        return
    
    # ä»¥å‰ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼ˆv1.0ã‚„simple_ensembleï¼‰ã¨æ¯”è¼ƒã®ææ¡ˆ
    print(f"\n\nğŸ” æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. ä»¥å‰ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ï¼ˆAUC 0.867ï¼‰ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª")
    print("2. v2.0è¨“ç·´æ™‚ã®ãƒ‡ãƒ¼ã‚¿ã¨ç‰¹å¾´é‡ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’èª¿æŸ»")
    print("3. å®Ÿéš›ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å†å®Ÿè£…")
    print("4. é©åˆ‡ãªæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´")

if __name__ == "__main__":
    test_v2_model()