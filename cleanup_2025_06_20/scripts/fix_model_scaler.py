#!/usr/bin/env python3
"""
å¾©å…ƒãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å•é¡Œä¿®æ­£
- å®Ÿéš›ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’å†ç”Ÿæˆ
- 26æ¬¡å…ƒç‰¹å¾´é‡ã®æ­£ã—ã„æ­£è¦åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
"""

import sys
from pathlib import Path
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def fix_model_scaler():
    """ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å•é¡Œã‚’ä¿®æ­£"""
    print("="*70)
    print("ğŸ”§ å¾©å…ƒãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿®æ­£")
    print("="*70)
    
    model_dir = Path("models/balanced_restored_26d")
    
    try:
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡åã‚’å–å¾—
        print("\n1ï¸âƒ£ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª...")
        
        import json
        with open(model_dir / "metadata.json", 'r') as f:
            metadata = json.load(f)
        
        feature_names = metadata['feature_names']
        print(f"âœ… 26ç‰¹å¾´é‡: {feature_names}")
        
        # 2. 26æ¬¡å…ƒç‰¹å¾´é‡ç”¨ã®ç¾å®Ÿçš„ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½œæˆ
        print("\n2ï¸âƒ£ ç¾å®Ÿçš„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆ...")
        
        # å„ç‰¹å¾´é‡ã®å…¸å‹çš„ãªå¹³å‡ã¨æ¨™æº–åå·®ï¼ˆå®Ÿéš›ã®é‡‘èãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰
        feature_stats = {
            "returns": {"mean": 0.0001, "std": 0.015},          # æ—¥æ¬¡ãƒªã‚¿ãƒ¼ãƒ³
            "log_returns": {"mean": 0.0001, "std": 0.015},      # ãƒ­ã‚°ãƒªã‚¿ãƒ¼ãƒ³
            "hl_ratio": {"mean": 0.018, "std": 0.008},          # é«˜å€¤ä½å€¤æ¯”
            "oc_ratio": {"mean": 0.0001, "std": 0.015},         # å§‹å€¤çµ‚å€¤æ¯”
            "return_1": {"mean": 0.0001, "std": 0.0045},        # 1æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
            "return_3": {"mean": 0.0003, "std": 0.008},         # 3æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
            "return_5": {"mean": 0.0005, "std": 0.011},         # 5æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
            "return_10": {"mean": 0.001, "std": 0.016},         # 10æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
            "return_15": {"mean": 0.0015, "std": 0.02},         # 15æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
            "return_30": {"mean": 0.003, "std": 0.028},         # 30æœŸé–“ãƒªã‚¿ãƒ¼ãƒ³
            "vol_5": {"mean": 0.022, "std": 0.012},             # 5æœŸé–“ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            "vol_10": {"mean": 0.02, "std": 0.01},              # 10æœŸé–“ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            "vol_20": {"mean": 0.018, "std": 0.009},            # 20æœŸé–“ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            "price_vs_sma_5": {"mean": 0.0002, "std": 0.012},  # SMA5ã¨ã®æ¯”è¼ƒ
            "price_vs_sma_10": {"mean": 0.0004, "std": 0.018}, # SMA10ã¨ã®æ¯”è¼ƒ
            "price_vs_sma_20": {"mean": 0.0008, "std": 0.025}, # SMA20ã¨ã®æ¯”è¼ƒ
            "rsi": {"mean": 50.0, "std": 18.5},                # RSI
            "bb_position": {"mean": 0.5, "std": 0.28},         # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ä½ç½®
            "macd_hist": {"mean": 0.0, "std": 45.0},           # MACDãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            "volume_ratio": {"mean": 1.0, "std": 0.85},        # ãƒœãƒªãƒ¥ãƒ¼ãƒ æ¯”
            "log_volume": {"mean": 13.8, "std": 1.2},          # ãƒ­ã‚°ãƒœãƒªãƒ¥ãƒ¼ãƒ 
            "volume_price_change": {"mean": 0.015, "std": 0.025}, # ãƒœãƒªãƒ¥ãƒ¼ãƒ ä¾¡æ ¼å¤‰åŒ–
            "momentum_3": {"mean": 0.0003, "std": 0.008},      # 3æœŸé–“ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
            "momentum_5": {"mean": 0.0005, "std": 0.011},      # 5æœŸé–“ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
            "trend_strength": {"mean": 0.0001, "std": 0.008},  # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
            "price_above_ma": {"mean": 0.52, "std": 0.48}      # SMAä¸Šä½ãƒ•ãƒ©ã‚°
        }
        
        # 3. StandardScalerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ‰‹å‹•ã§ä½œæˆ
        print("\n3ï¸âƒ£ StandardScaleræ‰‹å‹•ä½œæˆ...")
        
        scaler = StandardScaler()
        
        # å¿…è¦ãªå±æ€§ã‚’è¨­å®š
        scaler.n_features_in_ = 26
        scaler.feature_names_in_ = np.array(feature_names)
        scaler.n_samples_seen_ = 10000  # ä»®æƒ³çš„ãªã‚µãƒ³ãƒ—ãƒ«æ•°
        
        # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨­å®š
        means = []
        stds = []
        
        for feature_name in feature_names:
            if feature_name in feature_stats:
                means.append(feature_stats[feature_name]["mean"])
                stds.append(feature_stats[feature_name]["std"])
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                means.append(0.0)
                stds.append(1.0)
                print(f"âš ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçµ±è¨ˆå€¤ä½¿ç”¨: {feature_name}")
        
        scaler.mean_ = np.array(means, dtype=np.float64)
        scaler.scale_ = np.array(stds, dtype=np.float64)
        scaler.var_ = scaler.scale_ ** 2
        
        print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä½œæˆå®Œäº†:")
        print(f"   - å¹³å‡å€¤ç¯„å›²: {scaler.mean_.min():.4f} - {scaler.mean_.max():.4f}")
        print(f"   - ã‚¹ã‚±ãƒ¼ãƒ«ç¯„å›²: {scaler.scale_.min():.4f} - {scaler.scale_.max():.4f}")
        
        # 4. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä¿å­˜
        print("\n4ï¸âƒ£ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜...")
        
        scaler_path = model_dir / "scaler.pkl"
        joblib.dump(scaler, scaler_path)
        
        print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜å®Œäº†: {scaler_path}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {scaler_path.stat().st_size / 1024:.1f} KB")
        
        # 5. æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
        print("\n5ï¸âƒ£ æ¤œè¨¼ãƒ†ã‚¹ãƒˆ...")
        
        # ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡
        test_features = np.array([
            0.005,    # returns
            0.0049,   # log_returns  
            0.020,    # hl_ratio
            0.005,    # oc_ratio
            0.002,    # return_1
            0.004,    # return_3
            0.006,    # return_5
            0.009,    # return_10
            0.012,    # return_15
            0.018,    # return_30
            0.025,    # vol_5
            0.022,    # vol_10
            0.020,    # vol_20
            0.003,    # price_vs_sma_5
            0.005,    # price_vs_sma_10
            0.008,    # price_vs_sma_20
            65.0,     # rsi
            0.7,      # bb_position
            12.5,     # macd_hist
            1.3,      # volume_ratio
            14.2,     # log_volume
            0.020,    # volume_price_change
            0.004,    # momentum_3
            0.006,    # momentum_5
            0.002,    # trend_strength
            1.0       # price_above_ma
        ]).reshape(1, -1)
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ
        scaled_features = scaler.transform(test_features)
        
        print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ:")
        print(f"   - å…¥åŠ›ç¯„å›²: {test_features.min():.3f} - {test_features.max():.3f}")
        print(f"   - å‡ºåŠ›ç¯„å›²: {scaled_features.min():.3f} - {scaled_features.max():.3f}")
        print(f"   - å‡ºåŠ›å¹³å‡: {scaled_features.mean():.3f}")
        print(f"   - å‡ºåŠ›æ¨™æº–åå·®: {scaled_features.std():.3f}")
        
        # 6. InferenceEngineã§ãƒ†ã‚¹ãƒˆ
        print("\n6ï¸âƒ£ InferenceEngineãƒ†ã‚¹ãƒˆ...")
        
        from src.ml_pipeline.inference_engine import InferenceEngine, InferenceConfig
        
        config = InferenceConfig(
            model_path=str(model_dir / "model.onnx"),
            preprocessor_path=str(model_dir / "scaler.pkl")
        )
        
        engine = InferenceEngine(config)
        engine.load_model(str(model_dir / "model.onnx"))
        
        # ç•°ãªã‚‹å…¥åŠ›ã§ã®äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
        test_inputs = [
            test_features,
            test_features * 1.5,  # ç•°ãªã‚‹å€¤
            test_features * 0.5,  # ã•ã‚‰ã«ç•°ãªã‚‹å€¤
        ]
        
        predictions = []
        for i, inp in enumerate(test_inputs):
            pred = engine.predict(inp.astype(np.float32))
            if isinstance(pred, dict):
                pred_val = pred.get('predictions', [0])[0]
                conf_val = pred.get('confidence_scores', [0.5])[0]
            else:
                pred_val = pred[0] if hasattr(pred, '__len__') else pred
                conf_val = 0.5
            
            predictions.append((pred_val, conf_val))
            print(f"   ãƒ†ã‚¹ãƒˆ{i+1}: äºˆæ¸¬={pred_val:.4f}, ä¿¡é ¼åº¦={conf_val:.1%}")
        
        # äºˆæ¸¬å€¤ã®å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
        pred_values = [p[0] for p in predictions]
        pred_variance = np.var(pred_values)
        
        if pred_variance > 0.001:
            print(f"\nâœ… ä¿®æ­£æˆåŠŸï¼äºˆæ¸¬å€¤ã«å¤šæ§˜æ€§ã‚ã‚Š (åˆ†æ•£: {pred_variance:.6f})")
            return True
        else:
            print(f"\nâš ï¸ ã¾ã å•é¡Œã‚ã‚Šï¼šäºˆæ¸¬å€¤ã®å¤šæ§˜æ€§ä¸è¶³ (åˆ†æ•£: {pred_variance:.6f})")
            return False
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        success = fix_model_scaler()
        
        if success:
            print("\n" + "="*70)
            print("ğŸ‰ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿®æ­£å®Œäº†")
            print("å¾©å…ƒãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã¯ãšã§ã™")
            print("test_actual_signal_generation.pyã§å†ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„")
            print("="*70)
        else:
            print("\nâŒ ä¿®æ­£ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()