#!/usr/bin/env python3
"""
äºˆæ¸¬ã®åã‚Šã‚’èª¿æŸ»ã™ã‚‹ç°¡æ˜“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import numpy as np
import onnxruntime as ort
import json

def investigate_bias():
    """ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å‚¾å‘ã‚’èª¿æŸ»"""
    
    print("ğŸ” ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒã‚¤ã‚¢ã‚¹ã‚’èª¿æŸ»...\n")
    
    # 1. ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’èª­ã¿è¾¼ã¿
    model_path = "models/v3.1_improved/model.onnx"
    scaler_path = "models/v3.1_improved/manual_scaler.json"
    
    session = ort.InferenceSession(model_path)
    
    with open(scaler_path, 'r') as f:
        scaler_params = json.load(f)
    
    mean = np.array(scaler_params['means'])
    std = np.array(scaler_params['stds'])
    
    print(f"ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æƒ…å ±:")
    print(f"  ç‰¹å¾´é‡æ•°: {len(mean)}")
    print(f"  å¹³å‡å€¤ã®ç¯„å›²: [{np.min(mean):.4f}, {np.max(mean):.4f}]")
    print(f"  æ¨™æº–åå·®ã®ç¯„å›²: [{np.min(std):.4f}, {np.max(std):.4f}]")
    
    # stdãŒã‚¼ãƒ­ã¾ãŸã¯æ¥µå°ã®ç‰¹å¾´é‡ã‚’ç¢ºèª
    zero_std = np.sum(std == 0)
    tiny_std = np.sum((std > 0) & (std < 1e-6))
    print(f"  stdãŒã‚¼ãƒ­ã®ç‰¹å¾´é‡: {zero_std}å€‹")
    print(f"  stdãŒæ¥µå°(<1e-6)ã®ç‰¹å¾´é‡: {tiny_std}å€‹")
    
    # 2. æ§˜ã€…ãªå…¥åŠ›ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    print("\n\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ãƒ†ã‚¹ãƒˆ:")
    
    test_cases = [
        ("ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«", np.zeros(44)),
        ("å¹³å‡å€¤", mean),
        ("å¹³å‡å€¤ + 1std", mean + std),
        ("å¹³å‡å€¤ - 1std", mean - std),
        ("ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰", np.random.randn(44)),
        ("ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆä¸€æ§˜åˆ†å¸ƒï¼‰", np.random.uniform(-3, 3, 44)),
    ]
    
    predictions = []
    
    for name, features in test_cases:
        # æ­£è¦åŒ–
        std_safe = np.where(std == 0, 1.0, std)
        normalized = (features - mean) / std_safe
        
        # NaN/Infãƒã‚§ãƒƒã‚¯
        nan_count = np.isnan(normalized).sum()
        inf_count = np.isinf(normalized).sum()
        
        # äºˆæ¸¬
        input_data = normalized.reshape(1, -1).astype(np.float32)
        outputs = session.run(None, {'float_input': input_data})
        
        if len(outputs) > 1 and isinstance(outputs[1], list) and len(outputs[1]) > 0:
            prob_dict = outputs[1][0]
            prediction = prob_dict.get(1, 0.5)
        else:
            prediction = float(outputs[0][0])
        
        predictions.append(prediction)
        
        print(f"\n{name}:")
        print(f"  NaN/Inf: {nan_count}/{inf_count}")
        print(f"  äºˆæ¸¬å€¤: {prediction:.6f}")
        print(f"  æ–¹å‘: {'BUY' if prediction > 0.5 else 'SELL'}")
    
    # 3. äºˆæ¸¬å€¤ã®çµ±è¨ˆ
    print("\n\nğŸ“Š äºˆæ¸¬å€¤ã®çµ±è¨ˆ:")
    predictions = np.array(predictions)
    print(f"  æœ€å°å€¤: {np.min(predictions):.6f}")
    print(f"  æœ€å¤§å€¤: {np.max(predictions):.6f}")
    print(f"  å¹³å‡å€¤: {np.mean(predictions):.6f}")
    print(f"  æ¨™æº–åå·®: {np.std(predictions):.6f}")
    print(f"  å¤‰å‹•å¹…: {np.max(predictions) - np.min(predictions):.6f}")
    
    # 4. ç‰¹å®šã®ç‰¹å¾´é‡ã®å½±éŸ¿ã‚’èª¿æŸ»
    print("\n\nğŸ”¬ ç‰¹å¾´é‡ã®å½±éŸ¿åº¦èª¿æŸ»:")
    base_features = mean.copy()
    base_normalized = (base_features - mean) / std_safe
    base_pred = session.run(None, {'input': base_normalized.reshape(1, -1).astype(np.float32)})
    
    if len(base_pred) > 1 and isinstance(base_pred[1], list):
        base_value = base_pred[1][0].get(1, 0.5)
    else:
        base_value = float(base_pred[0][0])
    
    print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³äºˆæ¸¬: {base_value:.6f}")
    
    # å„ç‰¹å¾´é‡ã‚’å€‹åˆ¥ã«å¤‰æ›´ã—ã¦å½±éŸ¿ã‚’ç¢ºèª
    influences = []
    for i in range(44):
        if std[i] == 0:
            continue
            
        # +2stdã®å¤‰æ›´
        test_features = base_features.copy()
        test_features[i] += 2 * std[i]
        test_normalized = (test_features - mean) / std_safe
        test_pred = session.run(None, {'input': test_normalized.reshape(1, -1).astype(np.float32)})
        
        if len(test_pred) > 1 and isinstance(test_pred[1], list):
            test_value = test_pred[1][0].get(1, 0.5)
        else:
            test_value = float(test_pred[0][0])
        
        influence = abs(test_value - base_value)
        influences.append((i, influence))
    
    # å½±éŸ¿åº¦ã®å¤§ãã„ç‰¹å¾´é‡ãƒˆãƒƒãƒ—5
    influences.sort(key=lambda x: x[1], reverse=True)
    print("\nå½±éŸ¿åº¦ã®å¤§ãã„ç‰¹å¾´é‡ãƒˆãƒƒãƒ—5:")
    for idx, (feat_idx, influence) in enumerate(influences[:5]):
        print(f"  {idx+1}. ç‰¹å¾´é‡{feat_idx}: å½±éŸ¿åº¦ {influence:.6f}")
    
    # 5. çµè«–
    print("\n\nğŸ’¡ åˆ†æçµæœ:")
    if np.max(predictions) - np.min(predictions) < 0.1:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ç¯„å›²ãŒæ¥µç«¯ã«ç‹­ã„ï¼ˆ< 0.1ï¼‰")
        print("   â†’ ãƒ¢ãƒ‡ãƒ«ãŒé©åˆ‡ã«å­¦ç¿’ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§")
    
    if all(p < 0.5 for p in predictions):
        print("âŒ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§SELLäºˆæ¸¬")
        print("   â†’ ãƒ¢ãƒ‡ãƒ«ã«å¼·ã„ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹")
    
    if zero_std > 0 or tiny_std > 0:
        print(f"âš ï¸  æ¨™æº–åå·®ãŒç•°å¸¸ãªç‰¹å¾´é‡ãŒå­˜åœ¨ï¼ˆã‚¼ãƒ­: {zero_std}å€‹, æ¥µå°: {tiny_std}å€‹ï¼‰")
        print("   â†’ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®è¨ˆç®—ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§")

if __name__ == "__main__":
    investigate_bias()