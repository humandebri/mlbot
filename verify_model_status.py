#!/usr/bin/env python3
"""
ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ³ã‚’è©³ç´°æ¤œè¨¼
"""
import sys
sys.path.insert(0, '/home/ubuntu/mlbot')

import os
import json
import pickle
from pathlib import Path
from src.common.config import settings
from src.common.logging import get_logger
from src.common.discord_notifier import discord_notifier
from src.ml_pipeline.inference_engine import InferenceEngine, InferenceConfig

logger = get_logger(__name__)

def verify_model_status():
    """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ³ã‚’è©³ç´°æ¤œè¨¼"""
    
    logger.info("ğŸ” ãƒ¢ãƒ‡ãƒ«çŠ¶æ³ã®è©³ç´°æ¤œè¨¼é–‹å§‹...")
    
    discord_notifier.send_system_status(
        "model_verification",
        "ğŸ” **ãƒ¢ãƒ‡ãƒ«çŠ¶æ³æ¤œè¨¼é–‹å§‹** ğŸ”\n\n" +
        "ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿æ··å…¥å•é¡Œã®èª¿æŸ»ä¸­..."
    )
    
    verification_results = {}
    
    try:
        # 1. è¨­å®šç¢ºèª
        model_path = settings.model.model_path
        logger.info(f"ğŸ“‹ è¨­å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
        verification_results["config_path"] = model_path
        
        # 2. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        model_exists = os.path.exists(model_path)
        logger.info(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨: {model_exists}")
        verification_results["model_exists"] = model_exists
        
        if model_exists:
            model_size = os.path.getsize(model_path)
            logger.info(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {model_size:,} bytes")
            verification_results["model_size"] = model_size
        
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        metadata_path = Path(model_path).parent / "metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            logger.info(f"ğŸ“œ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata}")
            verification_results["metadata"] = metadata
        
        # 4. åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
        models_dir = Path("models")
        available_models = {}
        
        for model_dir in models_dir.iterdir():
            if model_dir.is_dir():
                model_onnx = model_dir / "model.onnx"
                if model_onnx.exists():
                    size = model_onnx.stat().st_size
                    available_models[model_dir.name] = {
                        "size": size,
                        "path": str(model_onnx)
                    }
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
                    meta_path = model_dir / "metadata.json"
                    if meta_path.exists():
                        with open(meta_path, 'r') as f:
                            meta = json.load(f)
                        available_models[model_dir.name]["metadata"] = meta
        
        logger.info(f"ğŸ“‚ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {list(available_models.keys())}")
        verification_results["available_models"] = available_models
        
        # 5. æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ
        try:
            inference_config = InferenceConfig(
                model_path=model_path,
                confidence_threshold=0.5
            )
            inference_engine = InferenceEngine(inference_config)
            inference_engine.load_model()
            
            logger.info("âœ… æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–æˆåŠŸ")
            verification_results["inference_engine"] = "success"
            
            # ãƒ¢ãƒ‡ãƒ«è©³ç´°å–å¾—
            if hasattr(inference_engine, 'onnx_session'):
                input_shape = inference_engine.onnx_session.get_inputs()[0].shape
                output_shape = inference_engine.onnx_session.get_outputs()[0].shape
                logger.info(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«å…¥åŠ›å½¢çŠ¶: {input_shape}")
                logger.info(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›å½¢çŠ¶: {output_shape}")
                verification_results["model_shapes"] = {
                    "input": input_shape,
                    "output": output_shape
                }
            
        except Exception as e:
            logger.error(f"âŒ æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            verification_results["inference_engine"] = f"error: {e}"
        
        # 6. å ±å‘Šæ›¸ç”Ÿæˆ
        report = "ğŸ” **ãƒ¢ãƒ‡ãƒ«çŠ¶æ³æ¤œè¨¼çµæœ** ğŸ”\n\n"
        
        # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«
        current_model_name = Path(model_path).parent.name
        report += f"ğŸ“‹ **ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«**: {current_model_name}\n"
        report += f"ğŸ“ **å­˜åœ¨**: {'âœ…' if model_exists else 'âŒ'}\n"
        
        if model_exists:
            report += f"ğŸ“Š **ã‚µã‚¤ã‚º**: {model_size/1024/1024:.1f}MB\n"
            
            if "metadata" in verification_results:
                meta = verification_results["metadata"]
                auc = meta.get("performance", {}).get("auc", "ä¸æ˜")
                features = meta.get("features", "ä¸æ˜")
                report += f"ğŸ¯ **AUC**: {auc}\n"
                report += f"ğŸ”¢ **ç‰¹å¾´é‡æ•°**: {features}\n"
        
        # åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
        report += f"\nğŸ“‚ **åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«**:\n"
        for model_name, model_info in available_models.items():
            size_mb = model_info["size"] / 1024 / 1024
            
            meta = model_info.get("metadata", {})
            auc = meta.get("performance", {}).get("auc", "ä¸æ˜")
            
            status = "ğŸŸ¢ ç¾åœ¨" if model_name == current_model_name else "â­"
            report += f"{status} **{model_name}**: {size_mb:.1f}MB, AUC {auc}\n"
        
        # æ¨å¥¨äº‹é …
        if current_model_name == "v2.0":
            report += f"\nâš ï¸ **è­¦å‘Š**: v2.0ãƒ¢ãƒ‡ãƒ«ã¯ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´\n"
            report += f"ğŸ“ˆ **æ¨å¥¨**: v3.1_improvedã«åˆ‡ã‚Šæ›¿ãˆï¼ˆAUC 0.838ï¼‰"
        elif current_model_name == "v3.1_improved":
            report += f"\nâœ… **è‰¯å¥½**: é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ä¸­"
        
        # Discordé€šçŸ¥
        if current_model_name == "v3.1_improved":
            discord_notifier.send_system_status("model_verification_good", report)
        else:
            discord_notifier.send_error("model_verification", report)
        
        return verification_results
        
    except Exception as e:
        logger.error(f"âŒ æ¤œè¨¼å¤±æ•—: {e}")
        discord_notifier.send_error("model_verification", f"æ¤œè¨¼å¤±æ•—: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    logger.info("Starting model verification")
    result = verify_model_status()
    logger.info("Verification complete")